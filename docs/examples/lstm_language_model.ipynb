{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Language Modeling\n",
    "\n",
    "In this example, we'll incorporate an LSTM that defines a language model into Nengo. We'll train the LSTM on the [Penn Treebank](https://catalog.ldc.upenn.edu/ldc99t42) (PTB) dataset, which is commonly used in the NLP community for developing language models (a bit like how MNIST is used as a starting point when building models for visual recognition tasks). The PTB dataset is also used in the Tensorflow's [NLP tutorials](https://www.tensorflow.org/tutorials/recurrent), which allows for a useful comparison of the differences between building a model into NengoDL versus defining it in Tensorflow alone. \n",
    "\n",
    "To provide some background, a [language model](https://en.wikipedia.org/wiki/Language_model) is a model that assigns probabilities to word sequences. We'll use an [Long Short-Term Memory](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) (LSTM) network to do this. LSTM's are increasingly being used in a wide variety of state-of-the-art natural language processing systems, so they are good tool to be familiar with and make use of.\n",
    "\n",
    "Finally, in contrast to the previous example illustrating how to incorporate a prebuilt Tensorflow model into Nengo, one purpose of this example is to explore the construction of a model that directly uses fairly low-level Tensorflow code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peterblouw/abr/nengo_dl/nengo_dl/__init__.py:19: UserWarning: No GPU support detected. It is recommended that you install tensorflow-gpu (`pip install tensorflow-gpu`).\n",
      "  warnings.warn(\"No GPU support detected. It is recommended that you \"\n",
      "/Users/peterblouw/abr/nengo/nengo/builder/builder.py:234: UserWarning: Type '<class 'nengo.learning_rules.PES'>' already has a builder. Overwriting.\n",
      "  % nengo_class)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import nengo\n",
    "import nengo_dl\n",
    "import nengo.spa as spa \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "\n",
    "from ptb_lm import PTBModel\n",
    "from helpers import has_punc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train an LSTM on the Penn Treebank dataset\n",
    "\n",
    "We've included code for a simple LSTM model implemented in TensorFlow in this repository. With pure TensorFlow code, it is straightfoward to learn a set of parameters offline; we can then load the resulting parameters into a TensorNode that incorporates the learned LSTM function in a way that is consistent with Nengo's use of a real-time simulator. Specifically, we'll introduce some logic for allowing the TensorNode to selectively make use of the learned LSTM function based on both an externally provided control signal and the amount of simulation time that has passed. This setup will allow us to provide some sequence of input words to the TensorNode, and then predict an indefinite number of subsequent words. \n",
    "\n",
    "First, though, we'll download the Penn Treebank dataset and use the LSTM model to compute some perplexity values on both the training data and the validation data (to see how well our model is able to generalize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ptb_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "dim = 300 # dimensionality of LSTM state\n",
    "\n",
    "# download the Penn Treebank dataset (about 100mb unzipped) and pretrained model\n",
    "def download(url, filename):\n",
    "    urlretrieve(url, filename)\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        f.extractall()\n",
    "    \n",
    "download(\"https://drive.google.com/uc?export=download&id=0BxRAh6Eg1us4VURvVmRrZ0Fmd3M\", \"ptb_data.zip\")\n",
    "download(\"https://drive.google.com/uc?export=download&id=0BxRAh6Eg1us4UkExX3NtcmpoR3c\", \"ptb_model.zip\")\n",
    "    \n",
    "# set the path to the data and initialize the model\n",
    "path = os.path.join(os.getcwd(), 'simple-examples/data')\n",
    "model = PTBModel(path=path, dim=dim)\n",
    "\n",
    "# either train the model (takes ~45 min) or load a saved model\n",
    "do_training = False\n",
    "if do_training:\n",
    "    model.train(rate=rate, epochs=5, b_size=20, n_steps=15)\n",
    "    model.save('ptb_model.ckpt')\n",
    "else:\n",
    "    model.load('ptb_model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the model's [perplexity](https://en.wikipedia.org/wiki/Perplexity) on the PTB dataset. This is essentially a numerical measure of how well the model is able to encode or capture the statistical regularities that are present in the data. Smaller perplexity values correspond to the model doing a better job of predicting these regularities.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will double-check that this is implemented correctly\n",
    "train_ppl = model.perplexity_eval(model.ptb['train_data'])\n",
    "valid_ppl = model.perplexity_eval(model.ptb['valid_data'])\n",
    "\n",
    "print('Perplexity on training dataset: ', train_ppl)\n",
    "print('Perplexity on validation dataset: ', valid_ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results suggest that we are overfitting (PTB is quite a small dataset), but that we have still learned a decent model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a class for using the trained LSTM in Nengo\n",
    "\n",
    "An LSTM can be thought of as a non-linear function that gets recursively applied over a sequence of inputs (i.e., at each step in the sequence, the function takes the current input and the previous state as its arguments). So, with that in mind, we can now define a class that will apply our learned LSTM function in the context of a Nengo model. \n",
    "\n",
    "To integrate this LSTM with a Nengo model, we'll create a list of variables that we want to load from the PTB model when "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the variables that we want to use from the trained LSTM in our TensorNode, then save in a checkpoint file\n",
    "var_names = ['embedding_matrix', 'rnn/basic_lstm_cell/kernel','rnn/basic_lstm_cell/bias', 'b_softmax', 'W_softmax']\n",
    "model.save_variables(var_names, 'lstm_params.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a callable class that contains the code that will implement our LSTM within a TensorNode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(object):\n",
    "    # defines a function that can act as the callable in a TensorNode in a Nengo model\n",
    "    def __init__(self, dim, vocab_size, update_interval=1):\n",
    "        # initialize with the state dimensionality, vocabulary size, and update interval\n",
    "        self.dim = dim\n",
    "        self.vsize = vocab_size\n",
    "        self.interval = update_interval\n",
    "        \n",
    "    def pre_build(self, *args):\n",
    "        # define the core variables for providing input and making predictions with the LSTM\n",
    "        self.embeddings = tf.Variable(tf.zeros([self.vsize, self.dim]), name='embedding_matrix')\n",
    "        self.b_softmax = tf.Variable(tf.zeros(self.vsize), name='b_softmax')\n",
    "        self.W_softmax = tf.Variable(tf.zeros([self.dim, self.vsize]), name='W_softmax')\n",
    "        \n",
    "        # cell and hidden state variables for LSTM, these get updated with each call\n",
    "        self.c = tf.Variable(tf.zeros((1, self.dim)), tf.float32, name='state_c')\n",
    "        self.h = tf.Variable(tf.zeros((1, self.dim)), tf.float32, name='state_h')\n",
    "        self.state = tf.contrib.rnn.LSTMStateTuple(self.c, self.h)\n",
    "        \n",
    "        # define the LSTM object\n",
    "        self.cell = tf.contrib.rnn.BasicLSTMCell(self.dim, state_is_tuple=True) \n",
    "        \n",
    "        # variables for doing appropriate control flow\n",
    "        self.cache = tf.Variable(0.0, name='cache')\n",
    "        self.repval = tf.Variable(0.0, name='repval')\n",
    "\n",
    "    def post_build(self, sess, rng):\n",
    "        # restore only the needed variables from a pretrained LSTM\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        restoring = [v for v in variables for n in var_names if n in v.name] \n",
    "        saver = tf.train.Saver(restoring)\n",
    "        saver.restore(sess, 'lstm_params.ckpt')\n",
    "\n",
    "    def __call__(self, t, x):\n",
    "        # two-dimensional input, first dimension is input word, second is control signal\n",
    "        # (we are disregarding the axis of the input tensor corresponding to batch size)\n",
    "        # if signal < 1, the input word is used; otherwise, the last prediction is used\n",
    "        inp = x[0, 0]\n",
    "        sig = x[0, 1]\n",
    "\n",
    "        # functions define branches in comp graph that get selected via update interval\n",
    "        # might be clearer/simpler to define these as static methods, not sure\n",
    "        def update():\n",
    "            # pick either input or cache to use for lookup, depending on control signal\n",
    "            self.idx = tf.cond(sig < 1, lambda: inp, lambda: self.cache)\n",
    "            self.idx = tf.cast(tf.reshape(self.idx, (1,1)), tf.int32)\n",
    "            emb = tf.nn.embedding_lookup(self.embeddings, self.idx)\n",
    "    \n",
    "            # run the LSTM for one step using input embedding and current cell state\n",
    "            outputs, next_state = tf.nn.dynamic_rnn(self.cell, emb, initial_state=self.state)\n",
    "            outputs = tf.reshape(outputs, [-1, self.dim])\n",
    "            \n",
    "            # compute probability distribution over vocab and predict argmax \n",
    "            logits = tf.nn.xw_plus_b(outputs, self.W_softmax, self.b_softmax)\n",
    "            probs = tf.nn.softmax(logits)\n",
    "            prediction = tf.cast(tf.argmax(probs[0]), tf.float32)\n",
    "\n",
    "            # update the LSTM cell state and the cache\n",
    "            self.c = tf.assign(self.c, next_state.c)\n",
    "            self.h = tf.assign(self.h, next_state.h)\n",
    "            self.cache = tf.assign(self.cache, prediction) \n",
    "            \n",
    "            # return the predicted item while requiring state assignments\n",
    "            with tf.control_dependencies([self.c, self.h, self.cache]):\n",
    "                return tf.reshape(prediction, (1, 1))\n",
    "            \n",
    "        def repeat():\n",
    "            # return the most recently returned value \n",
    "            return tf.reshape(self.repval, (1, 1))\n",
    "        \n",
    "        # check whether current time is a multiple of the update clock\n",
    "        check = tf.equal(tf.mod(tf.round(1000 * t), self.interval), 0)\n",
    "        output = tf.cond(tf.cast(check, tf.bool), update, repeat)\n",
    "        \n",
    "        # save the output to return again during repeat calls\n",
    "        self.repval = tf.assign(self.repval, output[0,0])\n",
    "        \n",
    "        # return the output while requiring repval assignment\n",
    "        with tf.control_dependencies([self.repval]):\n",
    "            return tf.reshape(output, (1,1))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When an instance of this class is called, it will return an output based on whether it's input specifies an 'encoding' regime, or a 'prediction' regime. During encoding, the input is used to update the internal state of the LSTM; during prediction, the input is ignored, and the internal state is used to produce a new output with each call that involves updating the LSTM. (the update interval sets how frequently the LSTM actually updates in relation to the amount of time simulated in Nengo).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a Nengo model\n",
    "\n",
    "Now, everything is in place to build a model in Nengo that uses an LSTM to predict the next words in a sequence. Since we would like our LSTM outputs be semantic pointers corresponding to specific words (i.e., SPs that can be used for downstream tasks), we'll convert the PTB model vocabulary into a SPA vocabulary. We'll also define some functions for determining the behavior of nodes the provide a control signal and map to items in a SPA vocabulary, respectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 234\n",
    "n_inputs = 6\n",
    "update_interval = 25 # milliseconds between LSTM updates\n",
    "vocab_size = model.ptb['vocabulary']\n",
    "\n",
    "lstm_func = LSTM(dim, vocab_size, update_interval)\n",
    "\n",
    "def control_signal(t, x):\n",
    "    # return enc signal for time to provide n_inputs \n",
    "    if t < (update_interval * n_inputs + 1) / 1000.0:\n",
    "        return 0\n",
    "    # return prediction signal\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "spa_vocab = model.build_spa_vocab(dim=64)\n",
    "    \n",
    "# for defining a node that maps from LSTM outputs to SPs\n",
    "def vocab_lookup(t, x):\n",
    "    word = model.id_to_word[int(x)]\n",
    "    # check for words that can't be SP keys\n",
    "    if has_punc(word):\n",
    "        return spa_vocab['UNK'].v\n",
    "    else:\n",
    "        return spa_vocab[word.upper()].v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create an array of word ids from a prompt to present to the model as input, and then create the model definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the input prompt to predict continuations for\n",
    "prompt = 'trading stopped in the afternoon because'\n",
    "ids = [model.word_to_id[w] for w in prompt.split()]\n",
    "input_ids = np.reshape(np.array(ids), (len(ids), 1))\n",
    "input_ids = np.vstack((input_ids, np.zeros((10, 1))))    \n",
    "    \n",
    "with nengo.Network(seed=seed) as net:\n",
    "    net.config[nengo.Connection].synapse = None\n",
    "    net.config[nengo.Ensemble].neuron_type = nengo.RectifiedLinear()\n",
    "    \n",
    "    inp = nengo.Node(nengo.processes.PresentInput(input_ids, update_interval / 1000))\n",
    "    sig = nengo.Node(control_signal, size_in=1, size_out=1)\n",
    "    lstm = nengo_dl.TensorNode(lstm_func, size_in=2, size_out=1)\n",
    "    \n",
    "    ens = nengo.Ensemble(50 * spa_vocab.dimensions, spa_vocab.dimensions)\n",
    "    cleanup = nengo.Node(vocab_lookup, size_in=1, size_out=spa_vocab.dimensions)\n",
    "\n",
    "    nengo.Connection(inp, lstm[0])\n",
    "    nengo.Connection(sig, lstm[1])\n",
    "    nengo.Connection(lstm, cleanup)\n",
    "    nengo.Connection(cleanup, ens)\n",
    "    \n",
    "    net.config[nengo.Probe].synapse = None\n",
    "    inp_probe = nengo.Probe(inp)\n",
    "    sig_probe = nengo.Probe(sig)\n",
    "    lang_probe = nengo.Probe(lstm)\n",
    "    clean_probe = nengo.Probe(cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the model's output\n",
    "\n",
    "We can now simulate our Nengo model for an arbitrary amount of time, and the LSTM we have incorporated with first encode the provided input, and then produce an indefinitely long continuation of this input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = nengo_dl.Simulator(net, seed=seed)\n",
    "sim.run(0.5)\n",
    "\n",
    "# indices for sampling the simulator output at update intervals\n",
    "inp_idxs = range(0, n_inputs * update_interval, update_interval)\n",
    "out_idxs = range(n_inputs * update_interval, 500, update_interval)\n",
    "\n",
    "# Text representation of the output of the Nengo model \n",
    "print('Prompt: ', [model.id_to_word[i[0]] for i in sim.data[inp_probe][inp_idxs]])\n",
    "print('Predicted continuation: ', [model.id_to_word[i[0]] for i in sim.data[lang_probe][out_idxs]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is clear, the model's predicticted continuation contain plausible, well-formed sequences of words. To ensure that the model is performing as well as our basic tensorflow implementation, we can compare its predictions on the same input.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prompt: ', prompt)\n",
    "print('Predicted continuation: ', model.predict(prompt, 14)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is apparent, Finally, we can plot a standard semantic graph of the outputs to visualize what our model is actually doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# pick a subset of the vocab that include the predicted words\n",
    "plot_vocab = spa_vocab.create_subset(['OF','THE','MARKET','UNK','RECENT','WEAKNESS','IN','DOLLAR','DOW',\n",
    "                                      'INDUSTRIALS','CLOSED','UP','TRANSACTION','FINANCIAL','ROSE','A'])\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Predicted Output Semantic Pointers')\n",
    "plt.plot(sim.trange()[n_inputs*update_interval:], nengo.spa.similarity(sim.data[clean_probe][n_inputs*update_interval:], plot_vocab))\n",
    "plt.legend(plot_vocab.keys, fontsize='small', bbox_to_anchor=(1.05, 1.0), ncol=2)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Similarity\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Incorporate synapses in the model, refactor code, edit text, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
