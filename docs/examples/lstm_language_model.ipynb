{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Language Modeling\n",
    "\n",
    "In this example, we'll incorporate an LSTM that defines a language model into Nengo. We'll train the LSTM on the [Penn Treebank](https://catalog.ldc.upenn.edu/ldc99t42) (PTB) dataset, which is commonly used in the NLP community for developing language models (a bit like how MNIST is used as a starting point when building models for visual recognition tasks). The PTB dataset is also used in one of Tensorflow [NLP tutorials](https://www.tensorflow.org/tutorials/recurrent), which allows for a useful comparison of the differences between building a model into Nengo DL versus defining it in Tensorflow alone. The text in PTB is drawn from Wall Street Journal articles, so it tends to have a topical focus on finance and business. \n",
    "\n",
    "To provide some background, a [language model](https://en.wikipedia.org/wiki/Language_model) is a model that assigns probabilities to word sequences. We'll use a [Long Short-Term Memory](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) (LSTM) network to do this. LSTMs are widely used in state-of-the-art natural language processing systems, so they are a good tool to be familiar with.\n",
    "\n",
    "Finally, in contrast to earlier examples, one purpose of this notebook is to explore the construction of a model that directly uses fairly low-level Tensorflow code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "import nengo\n",
    "import nengo_dl\n",
    "import nengo.spa as spa \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "\n",
    "from tfmodels.ptb import LanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train an LSTM on the Penn Treebank dataset\n",
    "\n",
    "We've included code for a simple LSTM model implemented in TensorFlow in this repository. With pure TensorFlow code, it is straightfoward to learn a set of parameters offline; we can then define a TensorNode in Nengo that incorporates our learned LSTM function in a way that is consistent with Nengo's use of a real-time simulator. Specifically, we'll introduce some logic for allowing Nengo to selectively make use of the learned LSTM function. This setup will allow us to provide some sequence of input words to the TensorNode, and then predict an indefinite number of subsequent words. \n",
    "\n",
    "First, we'll download the Penn Treebank dataset and use the LSTM model to compute some perplexity values on both the training data and the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, filename):\n",
    "    urlretrieve(url, filename)\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        f.extractall()\n",
    "\n",
    "download(\"https://drive.google.com/uc?export=download&id=0B1kesKuNtxuBcTEzc2xPd25ob2c\", \n",
    "         \"ptb_data.zip\")\n",
    "download(\"https://drive.google.com/uc?export=download&id=0B1kesKuNtxuBTHB2Ti02SWZCUmc\", \n",
    "         \"ptb_model.zip\")\n",
    "\n",
    "path = os.path.join(os.getcwd(), 'simple-examples/data')\n",
    "model = LanguageModel(path=path, dim=300)\n",
    "\n",
    "# either train the model (takes ~45 min) or load a saved model\n",
    "do_training = False\n",
    "if do_training:\n",
    "    model.train(rate=1.0, epochs=8, b_size=20, n_steps=15)\n",
    "    model.save('ptb_model.ckpt')\n",
    "else:\n",
    "    model.load('ptb_model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the model's [perplexity](https://en.wikipedia.org/wiki/Perplexity) on the PTB dataset. This is essentially a numerical measure of how well the model is able to encode or capture the statistical regularities that are present in the data. Smaller perplexity values correspond to the model doing a better job of predicting these regularities.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_training:\n",
    "    train_ppl = model.perplexity_eval(model.ptb['train_data'])\n",
    "    valid_ppl = model.perplexity_eval(model.ptb['valid_data'])\n",
    "\n",
    "    print('Perplexity on training dataset: ', train_ppl)\n",
    "    print('Perplexity on validation dataset: ', valid_ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results suggest that we are overfitting (PTB is quite a small dataset), but that we have still learned a decent model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a class for using the trained LSTM in Nengo\n",
    "\n",
    "An LSTM can be thought of as a non-linear function that gets recursively applied over a sequence of inputs (i.e., at each step in the sequence, the function takes the current input and the previous state as its arguments). So, with that in mind, we can now define a callable class that will apply our learned LSTM function in the context of a Nengo model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(object):\n",
    "    # defines a function that can act as the callable in a TensorNode in a Nengo model\n",
    "    def __init__(self, dim, vsize, update_interval=1, varlist=None, ckpt=None):\n",
    "        # initialize with state dimensionality, vocab size, and update interval\n",
    "        # varlist and ckpt allow for optional loading of pretrained parameters\n",
    "        self.dim = dim\n",
    "        self.vsize = vsize\n",
    "        self.interval = update_interval\n",
    "        self.varlist = varlist\n",
    "        self.ckpt = ckpt\n",
    "        \n",
    "    def pre_build(self, *args):\n",
    "        # define the core variables for providing input and making predictions with the LSTM\n",
    "        self.embeddings = tf.Variable(tf.zeros([self.vsize, self.dim]), name='embedding_matrix')\n",
    "        self.b_softmax = tf.Variable(tf.zeros(self.vsize), name='b_softmax')\n",
    "        self.W_softmax = tf.Variable(tf.zeros([self.dim, self.vsize]), name='W_softmax')\n",
    "        \n",
    "        # cell state and hidden state variables for LSTM, these get updated with each call\n",
    "        # (variable assignment doesn't work with LSTMStateTuples, so we assign to c and h)\n",
    "        self.c = tf.Variable(tf.zeros((1, self.dim)), tf.float32, name='state_c')\n",
    "        self.h = tf.Variable(tf.zeros((1, self.dim)), tf.float32, name='state_h')\n",
    "        self.state = tf.contrib.rnn.LSTMStateTuple(self.c, self.h)\n",
    "        \n",
    "        # define the LSTM cell\n",
    "        self.cell = tf.contrib.rnn.BasicLSTMCell(self.dim, state_is_tuple=True) \n",
    "        \n",
    "        # variables for doing appropriate control flow\n",
    "        self.cache = tf.Variable(0.0, name='cache')\n",
    "        self.repval = tf.Variable(0.0, name='repval')\n",
    "\n",
    "    def post_build(self, sess, rng):\n",
    "        # restore variables from ckpt if one is provided to constructor\n",
    "        if self.varlist and self.ckpt:\n",
    "            variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "            restoring = [v for v in variables for n in self.varlist if n in v.name] \n",
    "            saver = tf.train.Saver(restoring)\n",
    "            saver.restore(sess, self.ckpt)\n",
    "\n",
    "    def __call__(self, t, x):\n",
    "        # two-dimensional input, first dimension is input word, second is control signal\n",
    "        # (we are disregarding the axis of the input tensor corresponding to batch size)\n",
    "        # if signal < 1, the input word is used; otherwise, the last prediction is used\n",
    "        inp = x[0, 0]\n",
    "        sig = x[0, 1]\n",
    "\n",
    "        # functions define graph branches that execute based on mod(t, update_interval)\n",
    "        def update():\n",
    "            # pick either input or cache to use for lookup, depending on control signal\n",
    "            self.idx = tf.cond(sig < 1, lambda: inp, lambda: self.cache)\n",
    "            self.idx = tf.cast(tf.reshape(self.idx, (1,1)), tf.int32)\n",
    "            emb = tf.nn.embedding_lookup(self.embeddings, self.idx)\n",
    "    \n",
    "            # run the LSTM for one step using input embedding and current cell state\n",
    "            outputs, next_state = tf.nn.dynamic_rnn(self.cell, emb, initial_state=self.state)\n",
    "            outputs = tf.reshape(outputs, [-1, self.dim])\n",
    "            \n",
    "            # compute probability distribution over vocab and predict argmax \n",
    "            logits = tf.nn.xw_plus_b(outputs, self.W_softmax, self.b_softmax)\n",
    "            probs = tf.nn.softmax(logits)\n",
    "            prediction = tf.cast(tf.argmax(probs[0]), tf.float32)\n",
    "\n",
    "            # assignment ops for updating the LSTM cell state and the cache\n",
    "            self.c = tf.assign(self.c, next_state.c)\n",
    "            self.h = tf.assign(self.h, next_state.h)\n",
    "            self.cache = tf.assign(self.cache, prediction) \n",
    "            \n",
    "            # return the predicted item while requiring state assignments\n",
    "            with tf.control_dependencies([self.c, self.h, self.cache]):\n",
    "                return tf.reshape(prediction, (1, 1))\n",
    "            \n",
    "        def repeat():\n",
    "            # return the most recently returned value \n",
    "            return tf.reshape(self.repval, (1, 1))\n",
    "\n",
    "        # check whether current time is a multiple of the update interval\n",
    "        # rescale to integers to avoid floats breaking the mod function\n",
    "        check = tf.equal(tf.mod(tf.round(t * 1000), tf.round(self.interval * 1000)), 0)\n",
    "        output = tf.cond(tf.cast(check, tf.bool), update, repeat)\n",
    "        \n",
    "        # save the output to return again during repeat calls\n",
    "        self.repval = tf.assign(self.repval, output[0,0])\n",
    "        \n",
    "        # return the output while requiring repval assignment\n",
    "        with tf.control_dependencies([self.repval]):\n",
    "            return tf.reshape(output, (1,1))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When an instance of this class is called, it will return an output based on whether the control signal specifies an 'encoding' regime or a 'prediction' regime. During encoding, the input is used to update the internal state of the LSTM; during prediction, the input is ignored and the LSTM updates on the basis of its predicted output and internal state. The update interval sets how frequently the LSTM actually updates in relation to the amount of time simulated in Nengo.  \n",
    "\n",
    "To use the class with our learned model parameters, we'll have to specify which variables we want to load, and then save these to a checkpoint file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = 'lstm_params.ckpt'\n",
    "varlist = ['embedding_matrix', 'rnn/basic_lstm_cell/kernel','rnn/basic_lstm_cell/bias', \n",
    "           'b_softmax', 'W_softmax']\n",
    "\n",
    "model.save_variables(varlist, ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we create an instance of `LSTM`, we can pass this list of variables and the name of the checkpoint file to the constructor to load our saved relevant parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a Nengo model\n",
    "\n",
    "Everything is in place to build a model in Nengo that uses an LSTM to predict the next words in a sequence. Since we would like our LSTM outputs be semantic pointers corresponding to specific words (i.e., SPs that can be used for downstream tasks), we'll convert the PTB model vocabulary into a SPA vocabulary. We'll also define some functions for determining the behavior of nodes that control the LSTM and map its outputs onto these semantic pointers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_dim = 300\n",
    "spa_dim = 64\n",
    "seed = 234\n",
    "n_inputs = 6\n",
    "update_interval = 0.05\n",
    "vsize = model.vsize\n",
    "\n",
    "spa_vocab = model.build_spa_vocab(spa_dim)\n",
    "lstm_func = LSTM(lstm_dim, vsize, update_interval, varlist=varlist, ckpt=ckpt)\n",
    "\n",
    "def control_signal(t, x):\n",
    "    if t < update_interval * n_inputs + 0.001:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def vocab_lookup(t, x):\n",
    "    word = model.id_to_word[int(x)]\n",
    "    if model.has_punc(word):\n",
    "        return spa_vocab['UNK'].v\n",
    "    else:\n",
    "        return spa_vocab[word.upper()].v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now create an array of word ids to present to the model as input. Any prompt of length `n_inputs` can be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'trading stopped in the afternoon because'\n",
    "\n",
    "ids = [model.word_to_id[w] for w in prompt.split()]\n",
    "input_ids = np.reshape(np.array(ids), (len(ids), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a point of reference, here's what the original LSTM model predicts as a continuation for this prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(prompt, n_steps=45)\n",
    "\n",
    "print('Initial prompt: ', prompt)\n",
    "print('Continuation: ', ' '.join(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to define our Nengo model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo.Network(seed=seed) as net:\n",
    "    net.config[nengo.Connection].synapse = None\n",
    "    net.config[nengo.Probe].synapse = None\n",
    "    \n",
    "    inp = nengo.Node(nengo.processes.PresentInput(input_ids, update_interval))\n",
    "    sig = nengo.Node(control_signal, size_in=1, size_out=1)\n",
    "    lstm = nengo_dl.TensorNode(lstm_func, size_in=2, size_out=1)\n",
    "    \n",
    "    ens = nengo.Ensemble(50 * spa_vocab.dimensions, spa_vocab.dimensions)\n",
    "    lookup = nengo.Node(vocab_lookup, size_in=1, size_out=spa_vocab.dimensions)\n",
    "\n",
    "    nengo.Connection(inp, lstm[0])\n",
    "    nengo.Connection(sig, lstm[1])\n",
    "    nengo.Connection(lstm, lookup)\n",
    "    nengo.Connection(lookup, ens)\n",
    "    \n",
    "    p_inp = nengo.Probe(inp)\n",
    "    p_lstm= nengo.Probe(lstm)\n",
    "    p_lookup = nengo.Probe(lookup, synapse=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simulate the model and evaluate its output\n",
    "\n",
    "We can simulate our Nengo model for an arbitrary amount of time, and the incorporated LSTM will first encode the provided input before predicting continuations for as long as the simulator is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_time = 1\n",
    "\n",
    "with nengo_dl.Simulator(net, seed=seed) as sim:\n",
    "    sim.run(sim_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a little work, we can convert the probed simulation data into a text representation of the model's inputs and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_text():\n",
    "    # make indices for sampling the simulator output to collect text predictions\n",
    "    step_int = int(update_interval*1000)\n",
    "    max_step = int(sim_time*1000)\n",
    "    inp_idxs = range(0, n_inputs*step_int, step_int)\n",
    "    out_idxs = range(n_inputs*step_int, max_step, step_int)\n",
    "\n",
    "    # Print a text representation of the output of the Nengo model \n",
    "    print('Prompt: ', [model.id_to_word[i[0]] for i in sim.data[p_inp][inp_idxs]])\n",
    "    print('Continuation: ', [model.id_to_word[i[0]] for i in sim.data[p_lstm][out_idxs]])\n",
    "    \n",
    "collect_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is clear, the model's predicticted continuation is identical to that of our original tensorflow implementation. We can also plot a standard semantic pointer graph of the outputs to visualize what the model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(plot_keys, cols=1):\n",
    "    plot_vocab = spa_vocab.create_subset(plot_keys)\n",
    "    step_int = int(update_interval*1000)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title('Predicted Output Vectors')\n",
    "    plt.plot(sim.trange()[n_inputs*step_int:], \n",
    "             nengo.spa.similarity(sim.data[p_lookup][n_inputs*step_int:], plot_vocab))\n",
    "    plt.legend(plot_vocab.keys, fontsize='medium', bbox_to_anchor=(1.0, 1.0), ncol=cols)\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Similarity\");\n",
    "\n",
    "plot_keys = ['OF','THE','MARKET','UNK','RECENT','WEAKNESS','IN','DOLLAR',\n",
    "             'DOW','INDUSTRIALS','CLOSED']\n",
    "plot(plot_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is straightfoward to run the model for a longer period of time to collect further predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_time = 2\n",
    "with nengo_dl.Simulator(net, seed=seed) as sim:\n",
    "    sim.run(sim_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_keys = ['OF','THE','MARKET','UNK','RECENT','WEAKNESS','IN','DOLLAR','DOW','INDUSTRIALS',\n",
    "             'CLOSED','DOWN','OFF','CENTS','COMPANY','SAID','TRANSACTION','WAS','A','MILLION',\n",
    "             'LOANS','DEPOSITORY','INSTITUTION','BY','N','AT']\n",
    "\n",
    "plot(plot_keys, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to apply the ideas in this notebook during the creation of complex SPA models. For instance, one could provide the predicted output semantic pointers to an associative memory that maps them to structured representations of some kind. These structured representations could then be used to support further downstream tasks involving inference, memory, or motor planning. It is also possible to use a more complicated LSTM architecture as the callable in the model's TensorNode. Feel free to explore these possibilities on your own."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
