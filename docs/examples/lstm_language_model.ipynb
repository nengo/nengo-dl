{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Language Modeling\n",
    "\n",
    "In this example, we'll incorporate an LSTM that defines a language model into Nengo. We'll train the LSTM on the [Penn Treebank](https://catalog.ldc.upenn.edu/ldc99t42) (PTB) dataset, which is commonly used in the NLP community for developing language models (a bit like how MNIST is used as a starting point when building models for visual recognition tasks). The PTB dataset is also used in the Tensorflow's [NLP tutorials](https://www.tensorflow.org/tutorials/recurrent), which allows for a useful comparison of the differences between building a model into NengoDL versus defining it in Tensorflow alone. The text in PTB is drawn from Wall Street Journal articles, so it tends to have a topical focus on finance and business. \n",
    "\n",
    "To provide some background, a [language model](https://en.wikipedia.org/wiki/Language_model) is a model that assigns probabilities to word sequences. We'll use an [Long Short-Term Memory](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) (LSTM) network to do this. LSTM's are increasingly being used in a wide variety of state-of-the-art natural language processing systems, so they are good tool to be familiar with and make use of.\n",
    "\n",
    "Finally, in contrast to the previous example illustrating how to incorporate a prebuilt Tensorflow model into Nengo, one purpose of this example is to explore the construction of a model that directly uses fairly low-level Tensorflow code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "import nengo\n",
    "import nengo_dl\n",
    "import nengo.spa as spa \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "\n",
    "from models.ptb import LanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train an LSTM on the Penn Treebank dataset\n",
    "\n",
    "We've included code for a simple LSTM model implemented in TensorFlow in this repository. With pure TensorFlow code, it is straightfoward to learn a set of parameters offline; we can then load the resulting parameters into a TensorNode that incorporates the learned LSTM function in a way that is consistent with Nengo's use of a real-time simulator. Specifically, we'll introduce some logic for allowing the TensorNode to selectively make use of the learned LSTM function based on both an externally provided control signal and the amount of simulation time that has passed. This setup will allow us to provide some sequence of input words to the TensorNode, and then predict an indefinite number of subsequent words. \n",
    "\n",
    "First, though, we'll download the Penn Treebank dataset and use the LSTM model to compute some perplexity values on both the training data and the validation data (to see how well our model is able to generalize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download the Penn Treebank dataset (about 100mb unzipped) and pretrained model\n",
    "def download(url, filename):\n",
    "    urlretrieve(url, filename)\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        f.extractall()\n",
    "    \n",
    "download(\"https://drive.google.com/uc?export=download&id=0BxRAh6Eg1us4VURvVmRrZ0Fmd3M\", \"ptb_data.zip\")\n",
    "download(\"https://drive.google.com/uc?export=download&id=0BxRAh6Eg1us4UkExX3NtcmpoR3c\", \"ptb_model.zip\")\n",
    "    \n",
    "path = os.path.join(os.getcwd(), 'simple-examples/data')\n",
    "model = LanguageModel(path=path, dim=300)\n",
    "\n",
    "# either train the model (takes ~45 min) or load a saved model\n",
    "do_training = False\n",
    "if do_training:\n",
    "    model.train(rate=rate, epochs=5, b_size=20, n_steps=15)\n",
    "    model.save('ptb_model.ckpt')\n",
    "else:\n",
    "    model.load('ptb_model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the model's [perplexity](https://en.wikipedia.org/wiki/Perplexity) on the PTB dataset. This is essentially a numerical measure of how well the model is able to encode or capture the statistical regularities that are present in the data. Smaller perplexity values correspond to the model doing a better job of predicting these regularities.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will double-check that this is implemented correctly, skip for CI\n",
    "if do_training:\n",
    "    train_ppl = model.perplexity_eval(model.ptb['train_data'])\n",
    "    valid_ppl = model.perplexity_eval(model.ptb['valid_data'])\n",
    "\n",
    "    print('Perplexity on training dataset: ', train_ppl)\n",
    "    print('Perplexity on validation dataset: ', valid_ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results suggest that we are overfitting (PTB is quite a small dataset), but that we have still learned a decent model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a class for using the trained LSTM in Nengo\n",
    "\n",
    "An LSTM can be thought of as a non-linear function that gets recursively applied over a sequence of inputs (i.e., at each step in the sequence, the function takes the current input and the previous state as its arguments). So, with that in mind, we can now define a callable class that will apply our learned LSTM function in the context of a Nengo model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(object):\n",
    "    # defines a function that can act as the callable in a TensorNode in a Nengo model\n",
    "    def __init__(self, dim, vsize, update_interval=1, varlist=None, ckpt=None):\n",
    "        # initialize with state dimensionality, vocab size, and update interval\n",
    "        # load_vars and ckpt allow for optional loading of pretrained params\n",
    "        self.dim = dim\n",
    "        self.vsize = vsize\n",
    "        self.interval = update_interval\n",
    "        self.varlist = varlist\n",
    "        self.ckpt = ckpt\n",
    "        \n",
    "    def pre_build(self, *args):\n",
    "        # define the core variables for providing input and making predictions with the LSTM\n",
    "        self.embeddings = tf.Variable(tf.zeros([self.vsize, self.dim]), name='embedding_matrix')\n",
    "        self.b_softmax = tf.Variable(tf.zeros(self.vsize), name='b_softmax')\n",
    "        self.W_softmax = tf.Variable(tf.zeros([self.dim, self.vsize]), name='W_softmax')\n",
    "        \n",
    "        # cell and hidden state variables for LSTM, these get updated with each call\n",
    "        self.c = tf.Variable(tf.zeros((1, self.dim)), tf.float32, name='state_c')\n",
    "        self.h = tf.Variable(tf.zeros((1, self.dim)), tf.float32, name='state_h')\n",
    "        self.state = tf.contrib.rnn.LSTMStateTuple(self.c, self.h)\n",
    "        \n",
    "        # define the LSTM object\n",
    "        self.cell = tf.contrib.rnn.BasicLSTMCell(self.dim, state_is_tuple=True) \n",
    "        \n",
    "        # variables for doing appropriate control flow\n",
    "        self.cache = tf.Variable(0.0, name='cache')\n",
    "        self.repval = tf.Variable(0.0, name='repval')\n",
    "\n",
    "    def post_build(self, sess, rng):\n",
    "        # restore variables from ckpt if provided to constructor\n",
    "        if self.varlist and self.ckpt:\n",
    "            variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "            restoring = [v for v in variables for n in self.varlist if n in v.name] \n",
    "            saver = tf.train.Saver(restoring)\n",
    "            saver.restore(sess, self.ckpt)\n",
    "\n",
    "    def __call__(self, t, x):\n",
    "        # two-dimensional input, first dimension is input word, second is control signal\n",
    "        # (we are disregarding the axis of the input tensor corresponding to batch size)\n",
    "        # if signal < 1, the input word is used; otherwise, the last prediction is used\n",
    "        inp = x[0, 0]\n",
    "        sig = x[0, 1]\n",
    "\n",
    "        # functions define graph branches that execute based on mod(t, update_interval)\n",
    "        def update():\n",
    "            # pick either input or cache to use for lookup, depending on control signal\n",
    "            self.idx = tf.cond(sig < 1, lambda: inp, lambda: self.cache)\n",
    "            self.idx = tf.cast(tf.reshape(self.idx, (1,1)), tf.int32)\n",
    "            emb = tf.nn.embedding_lookup(self.embeddings, self.idx)\n",
    "    \n",
    "            # run the LSTM for one step using input embedding and current cell state\n",
    "            outputs, next_state = tf.nn.dynamic_rnn(self.cell, emb, initial_state=self.state)\n",
    "            outputs = tf.reshape(outputs, [-1, self.dim])\n",
    "            \n",
    "            # compute probability distribution over vocab and predict argmax \n",
    "            logits = tf.nn.xw_plus_b(outputs, self.W_softmax, self.b_softmax)\n",
    "            probs = tf.nn.softmax(logits)\n",
    "            prediction = tf.cast(tf.argmax(probs[0]), tf.float32)\n",
    "\n",
    "            # assignment ops for updating the LSTM cell state and the cache\n",
    "            self.c = tf.assign(self.c, next_state.c)\n",
    "            self.h = tf.assign(self.h, next_state.h)\n",
    "            self.cache = tf.assign(self.cache, prediction) \n",
    "            \n",
    "            # return the predicted item while requiring state assignments\n",
    "            with tf.control_dependencies([self.c, self.h, self.cache]):\n",
    "                return tf.reshape(prediction, (1, 1))\n",
    "            \n",
    "        def repeat():\n",
    "            # return the most recently returned value \n",
    "            return tf.reshape(self.repval, (1, 1))\n",
    "\n",
    "        # check whether current time is a multiple of the update interval\n",
    "        # rescale to integers to avoid floats breaking the mod function\n",
    "        check = tf.equal(tf.mod(tf.round(t * 1000), tf.round(self.interval * 1000)), 0)\n",
    "        output = tf.cond(tf.cast(check, tf.bool), update, repeat)\n",
    "        \n",
    "        # save the output to return again during repeat calls\n",
    "        self.repval = tf.assign(self.repval, output[0,0])\n",
    "        \n",
    "        # return the output while requiring repval assignment\n",
    "        with tf.control_dependencies([self.repval]):\n",
    "            return tf.reshape(output, (1,1))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When an instance of this class is called, it will return an output based on whether it's input specifies an 'encoding' regime, or a 'prediction' regime. During encoding, the input is used to update the internal state of the LSTM; during prediction, the input is ignored, and the internal state is used to produce a new output with each call that involves updating the LSTM. (the update interval sets how frequently the LSTM actually updates in relation to the amount of time simulated in Nengo).  \n",
    "\n",
    "To use the class with our learned model parameters, we'll have to specify which variables we want to load, and then save these to a checkpoint file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varlist = ['embedding_matrix', 'rnn/basic_lstm_cell/kernel','rnn/basic_lstm_cell/bias', 'b_softmax', 'W_softmax']\n",
    "ckpt = 'lstm_params.ckpt'\n",
    "\n",
    "model.save_variables(varlist, ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we create an instance of `LSTM`, we can pass this list of variables and the name of the checkpoint file to the constructor to load the relevant parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a Nengo model\n",
    "\n",
    "Now, everything is in place to build a model in Nengo that uses an LSTM to predict the next words in a sequence. Since we would like our LSTM outputs be semantic pointers corresponding to specific words (i.e., SPs that can be used for downstream tasks), we'll convert the PTB model vocabulary into a SPA vocabulary. We'll also define some functions for determining the behavior of nodes that control the LSTM and map its outputs onto specific semantic pointers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 300\n",
    "seed = 234\n",
    "n_inputs = 6\n",
    "update_interval = 0.05\n",
    "vsize = model.vsize\n",
    "\n",
    "spa_vocab = model.build_spa_vocab(dim=64)\n",
    "lstm_func = LSTM(dim, vsize, update_interval, varlist=varlist, ckpt=ckpt)\n",
    "\n",
    "def control_signal(t, x):\n",
    "    if t < update_interval * n_inputs + 0.001:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def vocab_lookup(t, x):\n",
    "    word = model.id_to_word[int(x)]\n",
    "    if model.has_punc(word):\n",
    "        return spa_vocab['UNK'].v\n",
    "    else:\n",
    "        return spa_vocab[word.upper()].v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now create an array of word ids to present to the model as input. Any prompt of length `n_inputs` can be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'trading stopped in the afternoon because'\n",
    "\n",
    "ids = [model.word_to_id[w] for w in prompt.split()]\n",
    "input_ids = np.reshape(np.array(ids), (len(ids), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a point of reference, here's what the original LSTM model predicts as a continuation for this prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(prompt, n_steps=45)\n",
    "\n",
    "print('Initial prompt: ', prompt)\n",
    "print('Predicted continuation: ', ' '.join(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to define our Nengo model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo.Network(seed=seed) as net:\n",
    "    net.config[nengo.Connection].synapse = None\n",
    "    net.config[nengo.Ensemble].neuron_type = nengo.RectifiedLinear()\n",
    "    \n",
    "    inp = nengo.Node(nengo.processes.PresentInput(input_ids, update_interval))\n",
    "    sig = nengo.Node(control_signal, size_in=1, size_out=1)\n",
    "    lstm = nengo_dl.TensorNode(lstm_func, size_in=2, size_out=1)\n",
    "    \n",
    "    ens = nengo.Ensemble(50 * spa_vocab.dimensions, spa_vocab.dimensions)\n",
    "    lookup = nengo.Node(vocab_lookup, size_in=1, size_out=spa_vocab.dimensions)\n",
    "\n",
    "    nengo.Connection(inp, lstm[0])\n",
    "    nengo.Connection(sig, lstm[1])\n",
    "    nengo.Connection(lstm, lookup)\n",
    "    nengo.Connection(lookup, ens)\n",
    "    \n",
    "    net.config[nengo.Probe].synapse = None\n",
    "    p_inp = nengo.Probe(inp)\n",
    "    p_lstm= nengo.Probe(lstm)\n",
    "    p_lookup = nengo.Probe(lookup, synapse=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simulate the model and evaluate its output\n",
    "\n",
    "We can now simulate our Nengo model for an arbitrary amount of time, and the LSTM we have incorporated with first encode the provided input, and then produce an indefinitely long continuation of this input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_int = int(update_interval*1000)\n",
    "sim_time = 1\n",
    "\n",
    "with nengo_dl.Simulator(net, seed=seed) as sim:\n",
    "    sim.run(sim_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a little work, we can convert the probed simulation data into a text representation of the model's inputs and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_text():\n",
    "    # make indices for sampling the simulator output to collect text predictions\n",
    "    step_int = int(update_interval*1000)\n",
    "    max_step = int(sim_time*1000)\n",
    "    inp_idxs = range(25, n_inputs*step_int, step_int)\n",
    "    out_idxs = range(n_inputs*step_int+25, max_step, step_int)\n",
    "\n",
    "    # Print a text representation of the output of the Nengo model \n",
    "    print('Prompt: ', [model.id_to_word[i[0]] for i in sim.data[p_inp][inp_idxs]])\n",
    "    print('Predicted continuation: ', [model.id_to_word[i[0]] for i in sim.data[p_lstm][out_idxs]])\n",
    "    \n",
    "collect_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is clear, the model's predicticted continuation is idential that of our original tensorflow implementation. We can also plot a standard semantic pointer graph of the outputs to visualize what the model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a subset of the vocab to plot that includes the predicted words\n",
    "def plot(plot_keys, cols=1):\n",
    "    plot_vocab = spa_vocab.create_subset(plot_keys)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title('Predicted Output Vectors')\n",
    "    plt.plot(sim.trange()[n_inputs*step_int:], \n",
    "             nengo.spa.similarity(sim.data[p_lookup][n_inputs*step_int:], plot_vocab))\n",
    "    plt.legend(plot_vocab.keys, fontsize='medium', bbox_to_anchor=(1.0, 1.0), ncol=cols)\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Similarity\");\n",
    "\n",
    "plot_keys = ['OF','THE','MARKET','UNK','RECENT','WEAKNESS','IN','DOLLAR','DOW','INDUSTRIALS','CLOSED']\n",
    "plot(plot_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is straightfoward to run the model for a longer period of time collection further predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_time = 2\n",
    "with nengo_dl.Simulator(net, seed=seed) as sim:\n",
    "    sim.run(sim_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_keys = ['OF','THE','MARKET','UNK','RECENT','WEAKNESS','IN','DOLLAR','DOW','INDUSTRIALS','CLOSED',\n",
    "             'DOWN','OFF','CENTS','COMPANY','SAID','TRANSACTION','WAS','A','MILLION','LOANS','DEPOSITORY',\n",
    "             'INSTITUTION','BY','N','AT']\n",
    "\n",
    "plot(plot_keys, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to apply the ideas in this notebook during the creation of complex SPA models. For instance, one could provide the predicted output semantic pointers to an associative memory that maps them to structured representations of some kind. These structured representations could then be used to support further downstread tasks involving inference, memory, or motor planning. Feel free to explore these possibilities on your own."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
