{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing a cognitive model with temporal dynamics\n",
    "\n",
    "[![Open In\n",
    "Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nengo/nengo-dl/blob/master/docs/examples/spa-memory.ipynb)\n",
    "\n",
    "In the previous examples we have essentially ignored time by defining models that map\n",
    "inputs to outputs in a single forward pass (e.g., we configured the default synapse to\n",
    "be `None`). In this example we'll introduce a simple process model of information\n",
    "retrieval based on [this](https://www.nengo.ai/nengo-spa/examples/question-memory.html)\n",
    "NengoSPA example. The idea is similar to [this\n",
    "example](https://www.nengo.ai/nengo-dl/examples/spa-retrieval.html) where we encoded\n",
    "role/filler information using semantic pointers and then retrieved a cued attribute.\n",
    "But in this example, rather than presenting the whole trace at once, we will present the\n",
    "input Role/Filler pairs one at a time and have the network remember them. Once all the\n",
    "bound pairs have been added to the memory, we can then query the model with a cue to\n",
    "test retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nengo\n",
    "from nengo import spa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import nengo_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing a memory network\n",
    "\n",
    "First we'll define a function for generating training data.  Note that this function\n",
    "will produce arrays of shape `(n_inputs, n_steps, dims)`, where `n_steps` will be the\n",
    "number of time steps in the process we want to model. To start, we'll generate simple\n",
    "examples in which the input trajectory consists of a single semantic pointer presented\n",
    "for some number of time steps, and the desired output trajectory involves maintaining a\n",
    "representation of that semantic pointer for some further number of time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_data(\n",
    "    n_inputs, vec_d, vocab_seed, presentation_time, delay_time, dt=0.001\n",
    "):\n",
    "    int_steps = int(presentation_time / dt)\n",
    "    mem_steps = int(delay_time / dt)\n",
    "    n_steps = int_steps + mem_steps\n",
    "\n",
    "    rng = np.random.RandomState(vocab_seed)\n",
    "    vocab = spa.Vocabulary(dimensions=vec_d, rng=rng, max_similarity=1)\n",
    "\n",
    "    # initialize arrays for input and output trajectories\n",
    "    inputs = np.zeros((n_inputs, n_steps, vec_d))\n",
    "    outputs = np.zeros((n_inputs, n_steps, vec_d))\n",
    "\n",
    "    # iterate through examples to be generated, fill arrays\n",
    "    for n in range(n_inputs):\n",
    "        name = f\"SP_{n}\"\n",
    "        vocab.add(name, vocab.create_pointer())\n",
    "\n",
    "        # create inputs and target memory for first pair\n",
    "        inputs[n, :int_steps, :] = vocab[name].v\n",
    "        outputs[n, :, :] = vocab[name].v\n",
    "\n",
    "    # make scaling ramp for target output trajectories\n",
    "    ramp = np.asarray([t / int_steps for t in range(int_steps)])\n",
    "    ramp = np.concatenate((ramp, np.ones(n_steps - int_steps)))\n",
    "    outputs = outputs * ramp[None, :, None]\n",
    "\n",
    "    return inputs, outputs, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first model will consist of a single input node and single recurrently connected\n",
    "memory ensemble. The input will present the input semantic pointer for a brief period,\n",
    "and then the task of the model will be to remember that semantic pointer over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "t_int = 0.01  # length of time for input presentation\n",
    "t_mem = 0.04  # length of time for the network to store the input\n",
    "dims = 32  # dimensionality of semantic pointer vectors\n",
    "n_neurons = 3 * dims  # number of neurons for memory ensemble\n",
    "minibatch_size = 32\n",
    "\n",
    "with nengo.Network(seed=seed) as net:\n",
    "    net.config[nengo.Ensemble].neuron_type = nengo.RectifiedLinear()\n",
    "    net.config[nengo.Ensemble].gain = nengo.dists.Choice([1])\n",
    "    net.config[nengo.Ensemble].bias = nengo.dists.Choice([0])\n",
    "\n",
    "    sp_input = nengo.Node(np.zeros(dims))\n",
    "    memory = nengo.Ensemble(n_neurons, dims)\n",
    "\n",
    "    tau = 0.01  # synaptic time constant on recurrent connection\n",
    "    nengo.Connection(sp_input, memory, transform=tau / t_int, synapse=tau)\n",
    "    nengo.Connection(memory, memory, transform=1, synapse=tau)\n",
    "\n",
    "    sp_probe = nengo.Probe(sp_input)\n",
    "    memory_probe = nengo.Probe(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll run the model for the specified length of time in order to see how well the\n",
    "memory works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test data\n",
    "test_inputs, test_targets, test_vocab = get_memory_data(\n",
    "    minibatch_size, dims, seed, t_int, t_mem\n",
    ")\n",
    "\n",
    "# run with one example input\n",
    "with nengo_dl.Simulator(net, seed=seed, minibatch_size=minibatch_size) as sim:\n",
    "    sim.run(t_int + t_mem, data={sp_input: test_inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_memory_example(plot_sim, vocab, example_input=0):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    name = f\"SP_{example_input}\"\n",
    "\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(\n",
    "        plot_sim.trange(),\n",
    "        nengo.spa.similarity(test_inputs[example_input], vocab),\n",
    "        color=\"black\",\n",
    "        alpha=0.2,\n",
    "    )\n",
    "    plt.plot(\n",
    "        plot_sim.trange(),\n",
    "        nengo.spa.similarity(test_inputs[example_input], vocab[name].v),\n",
    "        label=name,\n",
    "    )\n",
    "    plt.legend(fontsize=\"x-small\", loc=\"right\")\n",
    "    plt.ylim([-0.2, 1.1])\n",
    "    plt.ylabel(\"Input\")\n",
    "\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(\n",
    "        plot_sim.trange(),\n",
    "        nengo.spa.similarity(test_targets[example_input], vocab),\n",
    "        color=\"black\",\n",
    "        alpha=0.2,\n",
    "    )\n",
    "    plt.plot(\n",
    "        plot_sim.trange(),\n",
    "        nengo.spa.similarity(test_targets[example_input], vocab[name].v),\n",
    "        label=name,\n",
    "    )\n",
    "    plt.legend(fontsize=\"x-small\", loc=\"right\")\n",
    "    plt.ylim([-0.2, 1.1])\n",
    "    plt.ylabel(\"Target Memory\")\n",
    "\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(\n",
    "        plot_sim.trange(),\n",
    "        nengo.spa.similarity(plot_sim.data[memory_probe][example_input], vocab),\n",
    "        color=\"black\",\n",
    "        alpha=0.2,\n",
    "    )\n",
    "    plt.plot(\n",
    "        plot_sim.trange(),\n",
    "        nengo.spa.similarity(plot_sim.data[memory_probe][example_input], vocab[name].v),\n",
    "        label=name,\n",
    "    )\n",
    "    plt.legend(fontsize=\"x-small\", loc=\"right\")\n",
    "    plt.ylim([-0.2, 1.1])\n",
    "    plt.ylabel(\"Output Memory\")\n",
    "    plt.xlabel(\"time [s]\")\n",
    "\n",
    "\n",
    "plot_memory_example(sim, test_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots show the similarity of the input/target/output vectors to all the items in\n",
    "the vocabulary.  The similarity to the correct vocabulary item is highlighted, and we\n",
    "can see that while the memory is storing the correct item, that storage is not\n",
    "particularly stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve retention we can use NengoDL to fine tune the model parameters. Training on\n",
    "temporally extended trajectories can be slow, so we'll download pretrained parameters by\n",
    "default. You can train your own parameters by setting `do_training=True` (allowing you\n",
    "to vary things like learning rate or the number of training epochs to see the impact of\n",
    "those hyperparameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_training = False\n",
    "if do_training:\n",
    "    train_inputs, train_targets, _ = get_memory_data(4000, dims, seed, t_int, t_mem)\n",
    "\n",
    "    with nengo_dl.Simulator(net, minibatch_size=minibatch_size, seed=seed) as sim:\n",
    "        sim.compile(\n",
    "            optimizer=tf.optimizers.RMSprop(1e-4), loss={memory_probe: tf.losses.mse}\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"Test loss before:\",\n",
    "            sim.evaluate({sp_input: test_inputs}, {memory_probe: test_targets})[\"loss\"],\n",
    "        )\n",
    "\n",
    "        sim.fit({sp_input: train_inputs}, {memory_probe: train_targets}, epochs=100)\n",
    "\n",
    "        print(\n",
    "            \"Test loss after:\",\n",
    "            sim.evaluate({sp_input: test_inputs}, {memory_probe: test_targets})[\"loss\"],\n",
    "        )\n",
    "\n",
    "        sim.save_params(\"./mem_params\")\n",
    "else:\n",
    "    # download pretrained parameters\n",
    "    urlretrieve(\n",
    "        \"https://drive.google.com/uc?export=download&\"\n",
    "        \"id=1hwMoJ4RCm8-f4yN0kxdzl0ELvDqHuEUC\",\n",
    "        \"mem_params.npz\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo_dl.Simulator(net, seed=seed, minibatch_size=minibatch_size) as sim:\n",
    "    sim.load_params(\"./mem_params\")\n",
    "    sim.run(t_int + t_mem, data={sp_input: test_inputs})\n",
    "\n",
    "plot_memory_example(sim, test_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the training procedure significantly improves the stability of the\n",
    "memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Adding memory to the cognitive model\n",
    "\n",
    "Now we will return to the cued role/filler retrieval task from [this\n",
    "example](https://www.nengo.ai/nengo-dl/examples/spa-retrieval.html), and we will modify\n",
    "that task to include a memory aspect. Rather than presenting the complete trace as input\n",
    "all at once, we will present each $ROLE$/$FILLER$ pair one at a time.  The task of the\n",
    "network will be to bind each individual pair together, add them together to generate the\n",
    "full trace, store that trace in memory, and then when given one of the Roles as a cue,\n",
    "output the corresponding Filler.  For example, one pass through the task would consist\n",
    "of the following phases:\n",
    "\n",
    "| phase | role input | filler input | cue       | target output |\n",
    "|-------|------------|--------------|-----------|---------------|\n",
    "| 1     | $ROLE_0$   | $FILLER_0$   |     -     |       -       |\n",
    "| 2     | $ROLE_1$   | $FILLER_1$   |     -     |       -       |\n",
    "| ...   |     ...    |      ...     |    ...    |      ...      |\n",
    "| $n$   | $ROLE_n$   | $FILLER_n$   |     -     |       -       |\n",
    "| $n+1$ |      -     |       -      | $ROLE_x$  | $FILLER_x$    |\n",
    "\n",
    "First we will create a function to generate the input/target data for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binding_data(\n",
    "    n_items, pairs_per_item, vec_d, rng_seed, presentation_time, delay_time, dt=0.001\n",
    "):\n",
    "    int_steps = int(presentation_time / dt)\n",
    "    mem_steps = int(delay_time / dt)\n",
    "    n_steps = int_steps * pairs_per_item + mem_steps\n",
    "\n",
    "    rng = np.random.RandomState(rng_seed)\n",
    "    vocab = spa.Vocabulary(dimensions=vec_d, rng=rng, max_similarity=1)\n",
    "\n",
    "    # initialize arrays for input and output trajectories\n",
    "    roles = np.zeros((n_items, n_steps, vec_d))\n",
    "    fills = np.zeros((n_items, n_steps, vec_d))\n",
    "    cues = np.zeros((n_items, n_steps, vec_d))\n",
    "    binding = np.zeros((n_items, n_steps, vec_d))\n",
    "    mem = np.zeros((n_items, n_steps, vec_d))\n",
    "    output = np.zeros((n_items, n_steps, vec_d))\n",
    "\n",
    "    # iterate through examples to be generated, fill arrays\n",
    "    for n in range(n_items):\n",
    "        role_names = [f\"ROLE_{n}_{i}\" for i in range(pairs_per_item)]\n",
    "        filler_names = [f\"FILLER_{n}_{i}\" for i in range(pairs_per_item)]\n",
    "\n",
    "        # each role/filler pair is presented for presentation_time seconds\n",
    "        for i in range(pairs_per_item):\n",
    "            roles[n, i * int_steps : (i + 1) * int_steps] = vocab.parse(role_names[i]).v\n",
    "            fills[n, i * int_steps : (i + 1) * int_steps] = vocab.parse(\n",
    "                filler_names[i]\n",
    "            ).v\n",
    "            binding[n, i * int_steps : (i + 1) * int_steps] = vocab.parse(\n",
    "                f\"{role_names[i]}*{filler_names[i]}\"\n",
    "            ).v\n",
    "\n",
    "        # randomly select a cue\n",
    "        cue_idx = rng.randint(pairs_per_item)\n",
    "\n",
    "        # cue is presented during the memorization period\n",
    "        cues[n, -mem_steps:, :] = vocab[role_names[cue_idx]].v\n",
    "\n",
    "        # the goal is to output the associated filler during the\n",
    "        # memorization phase\n",
    "        # note: we use nan for the target prior to the memorization\n",
    "        # phase, to indicate that it doesn't matter what the network\n",
    "        # output is during that phase\n",
    "        output[n, -mem_steps:, :] = vocab[filler_names[cue_idx]].v\n",
    "        output[n, :-mem_steps, :] = np.nan\n",
    "\n",
    "    mem[...] = np.cumsum(binding, axis=1) * dt / presentation_time\n",
    "\n",
    "    return roles, fills, cues, binding, mem, output, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this more complicated model we'll add two circular convolution network to our\n",
    "previous memory model, one to convolve the role/filler inputs and one to deconvolve the\n",
    "cued answer from the memory trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "t_int = 0.01  # length of time to present each input pair\n",
    "t_mem = 0.03  # length of memorization period\n",
    "n_pairs = 2  # number of role/filler pairs in each input\n",
    "t_run = n_pairs * t_int + t_mem  # total task time\n",
    "dims = 64  # dimensionality of semantic pointer vectors\n",
    "neurons_per_dim = 3\n",
    "minibatch_size = 64\n",
    "\n",
    "with nengo.Network(seed=seed) as net:\n",
    "    net.config[nengo.Ensemble].neuron_type = nengo.RectifiedLinear()\n",
    "    net.config[nengo.Ensemble].gain = nengo.dists.Choice([1])\n",
    "    net.config[nengo.Ensemble].bias = nengo.dists.Choice([0])\n",
    "    net.config[nengo.Connection].synapse = None\n",
    "\n",
    "    role_inp = nengo.Node(np.zeros(dims))\n",
    "    fill_inp = nengo.Node(np.zeros(dims))\n",
    "    cue_inp = nengo.Node(np.zeros(dims))\n",
    "\n",
    "    # circular convolution network to combine roles/fillers\n",
    "    cconv = nengo.networks.CircularConvolution(neurons_per_dim, dims)\n",
    "    nengo.Connection(role_inp, cconv.input_a)\n",
    "    nengo.Connection(fill_inp, cconv.input_b)\n",
    "\n",
    "    # memory network to store the role/filler pairs\n",
    "    memory = nengo.Ensemble(neurons_per_dim * dims, dims)\n",
    "    tau = 0.01\n",
    "    nengo.Connection(cconv.output, memory, transform=tau / t_int, synapse=tau)\n",
    "    nengo.Connection(memory, memory, transform=1, synapse=tau)\n",
    "\n",
    "    # another circular convolution network to extract the cued filler\n",
    "    ccorr = nengo.networks.CircularConvolution(neurons_per_dim, dims, invert_b=True)\n",
    "    nengo.Connection(memory, ccorr.input_a)\n",
    "    nengo.Connection(cue_inp, ccorr.input_b)\n",
    "\n",
    "    conv_probe = nengo.Probe(cconv.output, label=\"conv_probe\")\n",
    "    memory_probe = nengo.Probe(memory, label=\"memory_probe\")\n",
    "    output_probe = nengo.Probe(ccorr.output, label=\"output_probe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same metric as in the previous [retrieval\n",
    "example](https://www.nengo.ai/nengo-dl/examples/spa-retrieval.html) in order to assess\n",
    "the accuracy of the system.  That is, we will say that the network has successfully\n",
    "retrieved the cued value if the output is more similar to the correct answer than to any\n",
    "other items in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, vocab, targets, t_step=-1):\n",
    "    # provide the probed output data, the vocab,\n",
    "    # the target vectors, and the time step at which to evaluate\n",
    "\n",
    "    # get output at the given time step\n",
    "    output = output[:, t_step, :]\n",
    "\n",
    "    # compute similarity between each output and vocab item\n",
    "    sims = np.dot(vocab.vectors, output.T)\n",
    "    idxs = np.argmax(sims, axis=0)\n",
    "\n",
    "    # check that the output is most similar to the target\n",
    "    acc = np.mean(np.all(vocab.vectors[idxs] == targets[:, -1], axis=1))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test data\n",
    "test_roles, test_fills, test_cues, _, _, test_targets, test_vocab = get_binding_data(\n",
    "    minibatch_size, n_pairs, dims, seed + 1, t_int, t_mem\n",
    ")\n",
    "test_inputs = {role_inp: test_roles, fill_inp: test_fills, cue_inp: test_cues}\n",
    "\n",
    "with nengo_dl.Simulator(net, seed=seed, minibatch_size=minibatch_size) as sim:\n",
    "    sim.run(t_run, data=test_inputs)\n",
    "\n",
    "print(\"Retrieval accuracy:\", accuracy(sim.data[output_probe], test_vocab, test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the initial retrieval accuracy of our model is poor.\n",
    "We can improve the performance of the model by optimizing its parameters using NengoDL.\n",
    "As before we will download pre-trained parameters to save time, but you can run the\n",
    "training yourself by setting `do_training=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_training = False\n",
    "if do_training:\n",
    "    # generate training data\n",
    "    (\n",
    "        train_roles,\n",
    "        train_fills,\n",
    "        train_cues,\n",
    "        train_binding,\n",
    "        train_memory,\n",
    "        train_targets,\n",
    "        _,\n",
    "    ) = get_binding_data(8000, n_pairs, dims, seed, t_int, t_mem)\n",
    "\n",
    "    # note: when training we'll add targets for the intermediate outputs\n",
    "    # as well, to help shape the training process\n",
    "    train_inputs = {role_inp: train_roles, fill_inp: train_fills, cue_inp: train_cues}\n",
    "    train_targets = {\n",
    "        output_probe: train_targets,\n",
    "        conv_probe: train_binding,\n",
    "        memory_probe: train_memory,\n",
    "    }\n",
    "\n",
    "    with nengo_dl.Simulator(net, minibatch_size=minibatch_size, seed=seed) as sim:\n",
    "\n",
    "        sim.compile(loss={output_probe: nengo_dl.losses.nan_mse})\n",
    "        print(\n",
    "            \"Test loss before:\",\n",
    "            sim.evaluate(test_inputs, {output_probe: test_targets})[\"loss\"],\n",
    "        )\n",
    "\n",
    "        sim.compile(\n",
    "            optimizer=tf.optimizers.RMSprop(1e-4),\n",
    "            loss=nengo_dl.losses.nan_mse,\n",
    "            loss_weights={output_probe: 1.0, conv_probe: 0.25, memory_probe: 0.25},\n",
    "        )\n",
    "        sim.fit(train_inputs, train_targets, epochs=10)\n",
    "\n",
    "        sim.compile(loss={output_probe: nengo_dl.losses.nan_mse})\n",
    "        print(\n",
    "            \"Test loss after:\",\n",
    "            sim.evaluate(test_inputs, {output_probe: test_targets})[\"loss\"],\n",
    "        )\n",
    "\n",
    "        sim.save_params(\"./mem_binding_params\")\n",
    "else:\n",
    "    # download pretrained parameters\n",
    "    urlretrieve(\n",
    "        \"https://drive.google.com/uc?export=download&\"\n",
    "        \"id=1FzfWsEjDISXiMlh1IEPhoim9yilT4Zj0\",\n",
    "        \"mem_binding_params.npz\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recomputing our accuracy measure on the test inputs demonstrates that our optimization\n",
    "procedure has significantly improved the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo_dl.Simulator(net, seed=seed, minibatch_size=minibatch_size) as sim:\n",
    "    sim.load_params(\"./mem_binding_params\")\n",
    "    sim.run(t_run, data=test_inputs)\n",
    "\n",
    "print(\"Retrieval accuracy:\", accuracy(sim.data[output_probe], test_vocab, test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can see that the output of the model is not perfect, our retrieval\n",
    "accuracy is much higher.  You can modify\n",
    "various parameters of the model, such as the number of dimensions or the number of\n",
    "role/filler inputs, in order to see how that impacts performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
