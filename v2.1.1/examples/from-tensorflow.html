
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Coming from TensorFlow to NengoDL &#8212; NengoDL documentation</title>
    <link rel="stylesheet" href="../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/normalize.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/bootstrap.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/mmenu.all.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/components.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/nengo.css" />

<!-- Google Tag Manager -->
<script>
 (function (w, d, s, l, i) {
   w[l] = w[l] || [];
   w[l].push({ "gtm.start": new Date().getTime(), event: "gtm.js" });
   var f = d.getElementsByTagName(s)[0],
       j = d.createElement(s),
       dl = l != "dataLayer" ? "&l=" + l : "";
   j.async = true;
   j.src = "https://www.googletagmanager.com/gtm.js?id=" + i + dl;
   f.parentNode.insertBefore(j, f);
 })(window, document, "script", "dataLayer", "GTM-KWCR2HN");
</script>
<!-- End Google Tag Manager -->
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<!--[if lte IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->
<script type="text/javascript" src="https://cdn.crate.io/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script type="text/javascript" src="../_static/js/modernizr.js"></script>
    <script type="text/javascript" src="../_static/js/underscore.min.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/js/searchtools.js"></script>
    <script type="text/javascript" src="../_static/js/webflow.js"></script>
    <script type="text/javascript" src="../_static/js/bootstrap.js"></script>
    <script type="text/javascript" src="../_static/js/mmenu.all.min.js"></script>
    <script type="text/javascript" src="../_static/js/fontawesome.js"></script>
    <script type="text/javascript" src="../_static/js/custom.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Inserting a TensorFlow network into a Nengo model" href="pretrained-model.html" />
    <link rel="prev" title="Coming from Nengo to NengoDL" href="from-nengo.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head><body>
<header class="header-nav">
  <div class="container">
    <div class="navbar w-nav" data-animation="default" data-collapse="small" data-contain="1" data-duration="400">
      <a class="brand w-nav-brand" href="https://www.nengo.ai/">
        <img src="https://www.nengo.ai/design/_images/general-full-light.svg" width="120">
      </a>
      <nav class="w-nav-menu main-nav" role="navigation">
        <div class="dropdownBackground">
          <span class="arrow"></span>
        </div>
        <ul id="menu-top-navigation" class="menu">
          <li class="navlink w-nav-link menu-item menu-item-has-children">
            <a href="https://www.nengo.ai/overview.html">Overview</a>
            <ul class="sub-menu">
                <li class="menu-item"><a href="https://www.nengo.ai/quickstart.html">Quick Start</a></li>
            </ul>
          </li>
          <li class="navlink w-nav-link menu-item menu-item-has-children">
            <a href="https://www.nengo.ai/documentation.html">Documentation</a>
            <table class="sub-menu">
              <tr>
                <td class="menu-heading">Core</td>
                <td class="menu-heading">Backends</td>
                <td class="menu-heading">Add-ons</td>
              </tr>
              <tr>
                <td class="menu-item"><a href="https://www.nengo.ai/nengo">Nengo</a></td>
                <td class="menu-item"><a href="https://www.nengo.ai/nengo-fpga">Nengo FPGA</a></td>
                <td class="menu-item"><a href="https://github.com/nengo/nengo-examples">Nengo Examples</a></td>
              </tr>
              <tr>
                <td class="menu-item"><a href="https://www.nengo.ai/nengo-dl/">Nengo DL</a></td>
                <td class="menu-item"><a href="https://www.nengo.ai/nengo-loihi">Nengo Loihi</a></td>
                <td class="menu-item"><a href="https://www.nengo.ai/nengo-extras/">Nengo Extras</a></td>
              </tr>
              <tr>
                <td class="menu-item"><a href="https://github.com/nengo/nengo-gui">Nengo GUI</a></td>
                <td class="menu-item"><a href="https://github.com/nengo/nengo-mpi">Nengo MPI</a></td>
                <td class="menu-item"><a href="https://arvoelke.github.io/nengolib-docs/">Nengo Lib</a></td>
              </tr>
              <tr>
                <td class="menu-item"><a href="https://www.nengo.ai/nengo-spa/">Nengo SPA</a></td>
                <td class="menu-item"><a href="https://github.com/nengo/nengo-ocl">Nengo OpenCL</a></td>
                <td class="menu-item"></td>
              </tr>
              <tr>
                <td class="menu-item"></td>
                <td class="menu-item"><a href="https://github.com/project-rig/nengo_spinnaker">Nengo SpiNNaker</a></td>
                <td class="menu-item"></td>
              </tr>
            </table>
          </li>
          <li class="navlink w-nav-link menu-item menu-item-has-children">
            <a href="https://www.nengo.ai/community.html">Community</a>
            <ul class="sub-menu">
              <li class="menu-item"><a href="https://forum.nengo.ai/">Forum</a></li>
              <li class="menu-item"><a href="https://www.nengo.ai/publications.html">Publications</a></li>
              <li class="menu-item"><a href="https://www.nengo.ai/summerschool.html">Summer School</a></li>
              <li class="menu-item"><a href="https://www.nengo.ai/videos.html">Videos</a></li>
              <li class="menu-item"><a href="https://www.nengo.ai/people.html">People</a></li>
            </ul>
          </li>
          <li class="navlink w-nav-link menu-item menu-item-has-children">
            <a href="https://www.nengo.ai/projects.html">Development</a>
            <ul class="sub-menu">
              <li class="menu-item"><a href="https://www.nengo.ai/contributing.html">Contributor Guide</a></li>
              <li class="menu-item"><a href="https://www.nengo.ai/projects.html">Ecosystem</a></li>
            </ul>
          </li>
          <li class="navlink w-nav-link menu-item"><a href="https://www.nengo.ai/README.html">About</a></li>
          <li class="btn-link w-nav-link menu-item"><a href="https://www.nengo.ai/download.html">Download</a></li>
        </ul>
      </nav>

      <div id="mobile-nav" class="mm-menu">
        <ul class="offcanvas-menu">
          <li class="navlink w-nav-link menu-item"><a href="https://www.nengo.ai/overview.html">Overview</a></li>
          <li class="navlink w-nav-link menu-item menu-item-has-children">
            <a href="https://www.nengo.ai/documentation.html">Documentation</a>
            <ul class="sub-menu">
              <li class="menu-item"><a href="https://www.nengo.ai/quickstart.html">Quick Start</a></li>
              <li class="menu-item"><a href="http://www.nengo.ai/nengo">NengoCore</a></li>
              <li class="menu-item"><a href="https://github.com/nengo/nengo-gui#nengo-gui">NengoGUI</a></li>
            </ul>
          </li>
          <li class="navlink w-nav-link menu-item menu-item-has-children">
            <a href="https://www.nengo.ai/community.html">Community</a>
            <ul class="sub-menu">
              <li class="menu-item"><a href="https://forum.nengo.ai/">Forum</a></li>
              <li class="menu-item"><a href="https://www.nengo.ai/summerschool.html">Summer School</a></li>
              <li class="menu-item"><a href="https://www.nengo.ai/people.html">People</a></li>
            </ul>
          </li>
          <li class="navlink w-nav-link menu-item menu-item-has-children">
            <a href="https://www.nengo.ai/projects.html">Development</a>
            <ul class="sub-menu">
              <li class="menu-item"><a href="https://www.nengo.ai/contributing.html">Contributor Guide</a></li>
              <li class="menu-item"><a href="https://www.nengo.ai/projects.html">Ecosystem</a></li>
            </ul>
          </li>
          <li class="navlink w-nav-link menu-item"><a href="https://www.nengo.ai/README.html">About</a></li>
          <li class="btn-link w-nav-link menu-item"><a href="https://www.nengo.ai/download.html">Download</a></li>
        </ul>
      </div>

      <a href="#mobile-nav" class="fa fa-bars mobile-nav-button"></a>
    </div>
  </div>
</header>

<div class="w-section section border-top">
  <div class="container">
    <div class="row" id="examples/from-tensorflow">
      <div class="col-md-4 col-lg-3">
        <aside class="wrapper-navleft"><div role="complementary" class="bs-docs-sidebar hidden-print" id="nav-affix">
  <ul class="bs-docs-sidenav bs-sidenav nav" role="complementary">
    <a href="../index.html">
      <img class="logo" src="https://www.nengo.ai/design/_images/nengo-dl-full-light.svg" alt="NengoDL" width="200" />
    </a>
    <ul class="toctree nav nav-list">
      <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user-guide.html">User guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference.html">API reference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../examples.html">Examples</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="from-nengo.html">Coming from Nengo to NengoDL</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Coming from TensorFlow to NengoDL</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#What-is-Nengo">What is Nengo</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Simulating-a-network">Simulating a network</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Spiking-networks">Spiking networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Inserting-TensorFlow-code">Inserting TensorFlow code</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Deep-learning-parameter-optimization">Deep learning parameter optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#NEF-parameter-optimization">NEF parameter optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Running-on-neuromorphic-hardware">Running on neuromorphic hardware</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="pretrained-model.html">Inserting a TensorFlow network into a Nengo model</a></li>
<li class="toctree-l2"><a class="reference internal" href="spiking-mnist.html">Optimizing spiking neural networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="spa-retrieval.html">Optimizing a cognitive model</a></li>
<li class="toctree-l2"><a class="reference internal" href="spa-memory.html">Optimizing a cognitive model with temporal dynamics</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../resources.html">Additional resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../project.html">Project information</a></li>
</ul>

    </ul>
  </ul>
</div>
          
        </aside>
      </div>
      <div class="col-md-8 col-lg-9">
        <div class="wrapper-content-right">
          
<div class="version-select-container">
    <div data-delay="0" class="w-dropdown">
        <div class="w-dropdown-toggle toggle">
            <div class="toggle-text">v2.1.1</div>
            <div class="w-icon-dropdown-toggle toggle-icon"></div>
        </div>
        <nav class="w-dropdown-list dropdown-list">
            
            
                <a href="../../examples/from-tensorflow.html" class="w-dropdown-link">latest</a>
            

            
                
                <div class="w-dropdown-link">v2.1.1</div>
                
            
                
                    <a href="../../v2.1.0/examples/from-tensorflow.html" class="w-dropdown-link">v2.1.0</a>
                
            
                
                    <a href="../../v2.0.0/examples/from-tensorflow.html" class="w-dropdown-link">v2.0.0</a>
                
            
                
                    <a href="../../v1.2.1/examples/from-tensorflow.html" class="w-dropdown-link">v1.2.1</a>
                
            
                
                    <a href="../../v1.2.0/examples/from-tensorflow.html" class="w-dropdown-link">v1.2.0</a>
                
            
                
                    <a href="../../v1.1.0/examples/from-tensorflow.html" class="w-dropdown-link">v1.1.0</a>
                
            
                
                    <a href="../../v1.0.0/examples/from-tensorflow.html" class="w-dropdown-link">v1.0.0</a>
                
            
                
                    <a href="../../v0.6.2/examples/from-tensorflow.html" class="w-dropdown-link">v0.6.2</a>
                
            
                
                    <a href="../../v0.6.1/examples/from-tensorflow.html" class="w-dropdown-link">v0.6.1</a>
                
            
                
                    <a href="../../v0.6.0/examples/from-tensorflow.html" class="w-dropdown-link">v0.6.0</a>
                
            
                
                    <a href="../../v0.5.2/examples/from-tensorflow.html" class="w-dropdown-link">v0.5.2</a>
                
            
                
                    <a href="../../v0.5.1/examples/from-tensorflow.html" class="w-dropdown-link">v0.5.1</a>
                
            
                
                    <a href="../../v0.5.0/examples/from-tensorflow.html" class="w-dropdown-link">v0.5.0</a>
                
            
                
                    <a href="../../v0.4.0/examples/from-tensorflow.html" class="w-dropdown-link">v0.4.0</a>
                
            
                
                    <a href="../../v0.3.1/examples/from-tensorflow.html" class="w-dropdown-link">v0.3.1</a>
                
            
                
                    <a href="../../v0.3.0/examples/from-tensorflow.html" class="w-dropdown-link">v0.3.0</a>
                
            
                
                    <a href="../../v0.2.0/examples/from-tensorflow.html" class="w-dropdown-link">v0.2.0</a>
                
            
        </nav>
    </div>
</div>

          
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 7ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<div class="section" id="Coming-from-TensorFlow-to-NengoDL">
<h1>Coming from TensorFlow to NengoDL<a class="headerlink" href="#Coming-from-TensorFlow-to-NengoDL" title="Permalink to this headline">¶</a></h1>
<p>NengoDL combines two frameworks: Nengo and TensorFlow. This tutorial is designed for people who are familiar with TensorFlow and looking to learn more about neuromorphic modelling with NengoDL. For the other approach, users familiar with Nengo looking to learn how to use NengoDL, check out <a class="reference external" href="https://www.nengo.ai/nengo-dl/examples/from-nengo.html">this tutorial</a>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">import</span> <span class="nn">gzip</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">from</span> <span class="nn">urllib.request</span> <span class="k">import</span> <span class="n">urlretrieve</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">nengo</span>
<span class="kn">from</span> <span class="nn">nengo.utils.matplotlib</span> <span class="k">import</span> <span class="n">rasterplot</span>
<span class="kn">import</span> <span class="nn">nengo_dl</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</pre></div>
</div>
</div>
<div class="section" id="What-is-Nengo">
<h2>What is Nengo<a class="headerlink" href="#What-is-Nengo" title="Permalink to this headline">¶</a></h2>
<p>We’ll start with the very basics, where you might be wondering what Nengo is and why you would want to use it. Nengo is a tool for constructing and simulating neural networks. That is, to some extent, the same purpose as TensorFlow (although TensorFlow is more of a general computational framework with neural network leanings). For example, here is how we might build a simple two layer auto-encoder network in TensorFlow:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">n_in</span> <span class="o">=</span> <span class="mi">784</span>
<span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">minibatch_size</span> <span class="o">=</span> <span class="mi">50</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span><span class="o">.</span><span class="n">as_default</span><span class="p">()</span> <span class="k">as</span> <span class="n">auto_graph</span><span class="p">:</span>
    <span class="c1"># input</span>
    <span class="n">tf_a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">minibatch_size</span><span class="p">,</span> <span class="n">n_in</span><span class="p">))</span>

    <span class="c1"># first layer</span>
    <span class="n">tf_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span>
        <span class="n">tf_a</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">glorot_uniform</span><span class="p">())</span>

    <span class="c1"># second layer</span>
    <span class="n">tf_c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span>
        <span class="n">tf_b</span><span class="p">,</span> <span class="n">n_in</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">glorot_uniform</span><span class="p">())</span>
</pre></div>
</div>
</div>
<p>And here is how we would build the same network architecture in Nengo:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Network</span><span class="p">()</span> <span class="k">as</span> <span class="n">auto_net</span><span class="p">:</span>
    <span class="c1"># input</span>
    <span class="n">nengo_a</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Node</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_in</span><span class="p">))</span>

    <span class="c1"># first layer</span>
    <span class="n">nengo_b</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Ensemble</span><span class="p">(</span>
        <span class="n">n_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">neuron_type</span><span class="o">=</span><span class="n">nengo</span><span class="o">.</span><span class="n">RectifiedLinear</span><span class="p">())</span>
    <span class="n">nengo</span><span class="o">.</span><span class="n">Connection</span><span class="p">(</span>
        <span class="n">nengo_a</span><span class="p">,</span> <span class="n">nengo_b</span><span class="o">.</span><span class="n">neurons</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">nengo_dl</span><span class="o">.</span><span class="n">dists</span><span class="o">.</span><span class="n">Glorot</span><span class="p">())</span>

    <span class="c1"># second layer</span>
    <span class="n">nengo_c</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Ensemble</span><span class="p">(</span>
        <span class="n">n_in</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">neuron_type</span><span class="o">=</span><span class="n">nengo</span><span class="o">.</span><span class="n">RectifiedLinear</span><span class="p">())</span>
    <span class="n">nengo</span><span class="o">.</span><span class="n">Connection</span><span class="p">(</span>
        <span class="n">nengo_b</span><span class="o">.</span><span class="n">neurons</span><span class="p">,</span> <span class="n">nengo_c</span><span class="o">.</span><span class="n">neurons</span><span class="p">,</span>
        <span class="n">transform</span><span class="o">=</span><span class="n">nengo_dl</span><span class="o">.</span><span class="n">dists</span><span class="o">.</span><span class="n">Glorot</span><span class="p">())</span>

    <span class="c1"># probes are used to collect data from the network</span>
    <span class="n">p_c</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Probe</span><span class="p">(</span><span class="n">nengo_c</span><span class="o">.</span><span class="n">neurons</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Note the basic similarities: an overall container (<code class="docutils literal notranslate"><span class="pre">tf.Graph</span></code> or <code class="docutils literal notranslate"><span class="pre">nengo.Network</span></code>), containing layers (Ensembles) of neurons connected by weights.</p>
<p>One difference you’ll note is that with Nengo we separate the creation of the layers and the creation of the connections between layers. This is because the connection structure in Nengo networks often has a lot more state and general complexity than in typical deep learning networks, so it is helpful to be able to control it independently (we’ll see examples of this later).</p>
<p>Another new object you may notice is the <code class="docutils literal notranslate"><span class="pre">nengo.Probe</span></code>. This is used to collect data from the simulation; by adding a probe to <code class="docutils literal notranslate"><span class="pre">nengo_c.neurons</span></code>, we are indicating that we want to collect the activities of those neurons when the simulation is running. You can think of this like the “fetch” arguments in a TensorFlow Session, except we’re explicitly defining which objects in the network we want to fetch during graph construction rather than at run time.</p>
<p>We will not go into a lot of detail on Nengo here; there is much more functionality available, but we will focus on the features most familiar or relevant to those coming from a TensorFlow background. For a more in-depth introduction to Nengo, check out the Nengo-specific <a class="reference external" href="https://www.nengo.ai/nengo/">documentation</a> and <a class="reference external" href="https://www.nengo.ai/nengo/examples.html">examples</a>.</p>
</div>
<div class="section" id="Simulating-a-network">
<h2>Simulating a network<a class="headerlink" href="#Simulating-a-network" title="Permalink to this headline">¶</a></h2>
<p>To simulate a TensorFlow network we create a <code class="docutils literal notranslate"><span class="pre">Session</span></code> and call <code class="docutils literal notranslate"><span class="pre">sess.run</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">auto_graph</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="n">tf_c</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">tf_a</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">minibatch_size</span><span class="p">,</span> <span class="n">n_in</span><span class="p">))})</span>
</pre></div>
</div>
</div>
<p>Again, accomplishing the same thing in Nengo bears many similarities. We create a <code class="docutils literal notranslate"><span class="pre">Simulator</span></code> and call <code class="docutils literal notranslate"><span class="pre">sim.run</span></code>:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">nengo_dl</span><span class="o">.</span><span class="n">Simulator</span><span class="p">(</span><span class="n">network</span><span class="o">=</span><span class="n">auto_net</span><span class="p">,</span>
                        <span class="n">minibatch_size</span><span class="o">=</span><span class="n">minibatch_size</span><span class="p">)</span> <span class="k">as</span> <span class="n">sim</span><span class="p">:</span>
    <span class="n">sim</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="p">{</span><span class="n">nengo_a</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">minibatch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_in</span><span class="p">))})</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Build finished in 0:00:00
Optimization finished in 0:00:00
|             Constructing graph: build stage (0%)             | ETA:  --:--:--
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/home/travis/build/nengo/nengo-dl/nengo_dl/simulator.py:131: UserWarning: No GPU support detected. It is recommended that you install tensorflow-gpu (`pip install tensorflow-gpu`).
  &#34;No GPU support detected. It is recommended that you &#34;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Construction finished in 0:00:00
Simulation finished in 0:00:00
</pre></div></div>
</div>
<p>One difference you may note is the <code class="docutils literal notranslate"><span class="pre">0.001</span></code> in the call to <code class="docutils literal notranslate"><span class="pre">sim.run</span></code>. This is specifying the length of time (in simulated seconds) that we want to simulate the network. 0.001 corresponds to one simulation timestep with the default <code class="docutils literal notranslate"><span class="pre">Simulator</span></code> <code class="docutils literal notranslate"><span class="pre">dt</span></code> of 0.001 (which is also why our input data has a shape of 1 in the second dimension).</p>
<p>This highlights a key difference between Nengo and TensorFlow. Nengo simulations are fundamentally temporal in nature; unlike TensorFlow where the graph simply represents an abstract set of computations, in Nengo we (almost) always think of the graph as representing a stateful neural simulation, where values are accumulated, updated, and communicated over time. This is not to say there is no overlap (we can create TensorFlow simulations that execute over time, and we can create Nengo simulations
without temporal dynamics), but this is a different way of thinking about computations that influences how we construct and simulate networks in Nengo.</p>
<p>More details on the NengoDL Simulator can be found in <a class="reference external" href="https://www.nengo.ai/nengo-dl/simulator.html">the user guide</a>.</p>
</div>
<div class="section" id="Spiking-networks">
<h2>Spiking networks<a class="headerlink" href="#Spiking-networks" title="Permalink to this headline">¶</a></h2>
<p>Although Nengo can be used to create TensorFlow-style networks, it has been primarily designed for a different style of modelling: “neuromorphic” networks. Neuromorphic networks include features drawn from biological neural networks, in an effort to understand or recreate the functionality of biological brains. Note that these models fall on a spectrum with standard artificial neural networks, with different approaches incorporating different biological features. But in general the structure and
parameterization of these networks often differs significantly from standard deep network architectures.</p>
<p>We touched on this above in the discussion of temporality, which is one common feature of neuromorphic networks. Another common characteristic is the use of more complicated neuron models, in particular spiking neurons. In contrast to “rate” neurons (like <code class="docutils literal notranslate"><span class="pre">relu</span></code>) that output a continuous value, spiking neurons communicate via discrete bursts of output called spikes.</p>
<p>We can visualize this difference with a simple 1-layer network:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Network</span><span class="p">()</span> <span class="k">as</span> <span class="n">net</span><span class="p">:</span>
    <span class="c1"># our input node will output a sine wave with a period of 1 second</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Node</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">t</span><span class="p">))</span>

    <span class="c1"># we&#39;ll create one ensemble with rate neurons</span>
    <span class="n">b_rate</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Ensemble</span><span class="p">(</span>
        <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">neuron_type</span><span class="o">=</span><span class="n">nengo</span><span class="o">.</span><span class="n">RectifiedLinear</span><span class="p">(),</span> <span class="n">seed</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">nengo</span><span class="o">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b_rate</span><span class="p">)</span>

    <span class="c1"># and another ensemble with spiking neurons</span>
    <span class="n">b_spike</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Ensemble</span><span class="p">(</span>
        <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">neuron_type</span><span class="o">=</span><span class="n">nengo</span><span class="o">.</span><span class="n">SpikingRectifiedLinear</span><span class="p">(),</span> <span class="n">seed</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">nengo</span><span class="o">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b_spike</span><span class="p">)</span>

    <span class="n">p_a</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Probe</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">p_rate</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Probe</span><span class="p">(</span><span class="n">b_rate</span><span class="o">.</span><span class="n">neurons</span><span class="p">)</span>
    <span class="n">p_spike</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Probe</span><span class="p">(</span><span class="n">b_spike</span><span class="o">.</span><span class="n">neurons</span><span class="p">)</span>

<span class="k">with</span> <span class="n">nengo_dl</span><span class="o">.</span><span class="n">Simulator</span><span class="p">(</span><span class="n">net</span><span class="p">)</span> <span class="k">as</span> <span class="n">sim</span><span class="p">:</span>
    <span class="c1"># simulate the model for 1 second</span>
    <span class="n">sim</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">trange</span><span class="p">(),</span> <span class="n">sim</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">p_a</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;time&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;input value&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">trange</span><span class="p">(),</span> <span class="n">sim</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">p_rate</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;time&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;firing rate&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;b_rate&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">rasterplot</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">trange</span><span class="p">(),</span> <span class="n">sim</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">p_spike</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;time&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;neuron&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;b_spike&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Build finished in 0:00:00
Optimization finished in 0:00:00
Construction finished in 0:00:00
Simulation finished in 0:00:00
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_from-tensorflow_13_1.png" src="../_images/examples_from-tensorflow_13_1.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_from-tensorflow_13_2.png" src="../_images/examples_from-tensorflow_13_2.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_from-tensorflow_13_3.png" src="../_images/examples_from-tensorflow_13_3.png" />
</div>
</div>
<p>Each neuron responds to the input signal differently due to the random parameterization in the network (e.g.&nbsp;connection weights and biases). We have matched the parameterization in the rate and spiking ensembles so that it is easier to see the parallels.</p>
<p>Note that the same information is being represented in the two ensembles. For example, when the second neuron (orange) is outputting a high continuous value (in the second graph), the corresponding spiking neuron is outputting more discrete spikes (orange lines in the third graph).</p>
<p>We can see the parallels more clearly if we introduce another Nengo feature, synaptic filters. This is inspired by a biological feature where discrete spikes induce a continuous electrical waveform in the receiving neuron, at the synapse (the point where the two neurons connect). But computationally we can think of this simply as applying a filter to the spiking signal.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># nengo uses a linear lowpass filter by default</span>
<span class="n">filt</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Lowpass</span><span class="p">(</span><span class="n">tau</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>

<span class="c1"># apply filter to ensemble output spikes</span>
<span class="n">filtered_spikes</span> <span class="o">=</span> <span class="n">filt</span><span class="o">.</span><span class="n">filt</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">p_spike</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">trange</span><span class="p">(),</span> <span class="n">filtered_spikes</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;time&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;filtered spike train (firing rates)&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_from-tensorflow_15_0.png" src="../_images/examples_from-tensorflow_15_0.png" />
</div>
</div>
<p>We can see how the spike trains, when viewed through a synaptic filter, approximate the continuous rate values in the second graph above.</p>
<p>In this example we have computed the filtered signal manually for demonstration purposes, but in a typical Nengo model these synaptic filters are applied throughout the model, on the <code class="docutils literal notranslate"><span class="pre">Connection</span></code> objects. For example, the above filtering would be equivalent to <code class="docutils literal notranslate"><span class="pre">nengo.Connection(b_spike.neurons,</span> <span class="pre">x,</span> <span class="pre">synapse=0.05)</span></code> (from the perspective of a hypothetical downstream object <code class="docutils literal notranslate"><span class="pre">x</span></code>).</p>
<p>This is a helpful duality to keep in mind when coming to neuromorphic modelling and Nengo from a standard deep network background. Although spiking neurons seem like a radically different paradigm, they can compute and communicate the same information as their rate counterparts. But note that this only makes sense when we think of the network temporally (neurons spiking and being filtered over time).</p>
<p>There are many other neuron types built into Nengo (see <a class="reference external" href="https://www.nengo.ai/nengo/frontend_api.html#neuron-types">the documentation</a> for a complete list). These neuron models have various different behaviours, and managing their parameterization and simulation is an important part of Nengo’s design.</p>
</div>
<div class="section" id="Inserting-TensorFlow-code">
<h2>Inserting TensorFlow code<a class="headerlink" href="#Inserting-TensorFlow-code" title="Permalink to this headline">¶</a></h2>
<p>The goal of NengoDL is not to replace TensorFlow or Nengo, but to allow them to smoothly work together. Thus one important feature is the ability to write TensorFlow code directly, and insert it into a Nengo network. This allows us to use whichever framework is best suited for different parts of a model.</p>
<p>This functionality is accessed through the <code class="docutils literal notranslate"><span class="pre">nengo_dl.TensorNode</span></code> class. This allows us to wrap TensorFlow code in a Nengo object, so that it can easily communicate with the rest of a Nengo model. The TensorFlow code is written in a function that takes <code class="docutils literal notranslate"><span class="pre">tf.Tensors</span></code> as input, applies the desired manipulations through TensorFlow operations, and returns a <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code>. We then pass that function to the TensorNode.</p>
<p>For simple cases we can use <code class="docutils literal notranslate"><span class="pre">nengo_dl.tensor_layer</span></code>. This is a simplified interface for constructing <code class="docutils literal notranslate"><span class="pre">TensorNodes</span></code> that mimics the common layer-based API of deep learning frameworks (such as <code class="docutils literal notranslate"><span class="pre">tf.layers</span></code>). For example, suppose we want to apply batch normalization to the output of one of the Nengo ensembles. There is no built-in way to do batch normalization in Nengo, so we can instead turn to TensorFlow for this part of the model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">net</span><span class="p">:</span>
    <span class="n">batch_norm</span> <span class="o">=</span> <span class="n">nengo_dl</span><span class="o">.</span><span class="n">tensor_layer</span><span class="p">(</span>
        <span class="n">b_rate</span><span class="o">.</span><span class="n">neurons</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
    <span class="n">p_batch_norm</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Probe</span><span class="p">(</span><span class="n">batch_norm</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>This is essentially equivalent to the TensorFlow function <code class="docutils literal notranslate"><span class="pre">tf.layers.batch_normalization(b_rate.neurons,</span> <span class="pre">momentum=0.9)</span></code>, except it works with Nengo objects. For example, <code class="docutils literal notranslate"><span class="pre">b_rate</span></code> is a <code class="docutils literal notranslate"><span class="pre">nengo.Ensemble</span></code> in this case, and we can add Probes or Connections to <code class="docutils literal notranslate"><span class="pre">batch_norm</span></code> in the same way as any other Nengo object.</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">nengo_dl.tensor_layer</span></code> is simply a shortcut for creating a <code class="docutils literal notranslate"><span class="pre">TensorNode</span></code> and <code class="docutils literal notranslate"><span class="pre">Connection</span></code>; the above is equivalent to</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">net</span><span class="p">:</span>
    <span class="n">batch_norm</span> <span class="o">=</span> <span class="n">nengo_dl</span><span class="o">.</span><span class="n">TensorNode</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">),</span>
        <span class="n">size_in</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">nengo</span><span class="o">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">b_rate</span><span class="o">.</span><span class="n">neurons</span><span class="p">,</span> <span class="n">batch_norm</span><span class="p">,</span> <span class="n">synapse</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">p_batch_norm</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Probe</span><span class="p">(</span><span class="n">batch_norm</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>In general, we can use any function (a built in TensorFlow function or one we write ourselves) in a TensorNode. It must accept two parameters, <code class="docutils literal notranslate"><span class="pre">t</span></code> and <code class="docutils literal notranslate"><span class="pre">x</span></code>, where <code class="docutils literal notranslate"><span class="pre">t</span></code> is the current simulation time and <code class="docutils literal notranslate"><span class="pre">x</span></code> is the value of any Connections incoming to the TensorNode. Note that in the case of <code class="docutils literal notranslate"><span class="pre">tensor_layers</span></code> the <code class="docutils literal notranslate"><span class="pre">t</span></code> parameter is omitted. <code class="docutils literal notranslate"><span class="pre">x</span></code> will have shape <code class="docutils literal notranslate"><span class="pre">(minibatch_size,</span> <span class="pre">size_in)</span></code>, where <code class="docutils literal notranslate"><span class="pre">size_in</span></code> is the dimensionality of the input Connections to the node (specified in the
<code class="docutils literal notranslate"><span class="pre">size_in=10</span></code> argument above). The <code class="docutils literal notranslate"><span class="pre">TensorNode</span></code>/<code class="docutils literal notranslate"><span class="pre">tensor_layer</span></code> function should return a <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code> with shape <code class="docutils literal notranslate"><span class="pre">(minibatch_size,</span> <span class="pre">size_out)</span></code>, where <code class="docutils literal notranslate"><span class="pre">size_out</span></code> is the output dimensionality of the node (dependent on the manipulations applied to the inputs <code class="docutils literal notranslate"><span class="pre">x</span></code>). We could explicitly specify <code class="docutils literal notranslate"><span class="pre">size_out=10</span></code> in the above example, or if we don’t specify the output size it will be determined automatically by calling the node function with placeholder inputs.</p>
<p>Here is a simple network to illustrate a TensorNode’s input and output:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Network</span><span class="p">()</span> <span class="k">as</span> <span class="n">net</span><span class="p">:</span>
    <span class="c1"># node to provide an input value for the TensorNode</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Node</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">])</span>

    <span class="c1"># a TensorNode function to illustrate i/o</span>
    <span class="k">def</span> <span class="nf">tensor_func</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># print out the value of inputs t and x</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">nengo_dl</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">print_op</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="s2">&quot;t:&quot;</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">t</span><span class="p">]):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">nengo_dl</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">print_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s2">&quot;x:&quot;</span><span class="p">)</span>

        <span class="c1"># output t + x</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="c1"># create the TensorNode</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">nengo_dl</span><span class="o">.</span><span class="n">TensorNode</span><span class="p">(</span><span class="n">tensor_func</span><span class="p">,</span> <span class="n">size_in</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">nengo</span><span class="o">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">synapse</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

    <span class="n">p</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Probe</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="k">with</span> <span class="n">nengo_dl</span><span class="o">.</span><span class="n">Simulator</span><span class="p">(</span><span class="n">net</span><span class="p">)</span> <span class="k">as</span> <span class="n">sim</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TensorNode input:&quot;</span><span class="p">)</span>
    <span class="n">sim</span><span class="o">.</span><span class="n">run_steps</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TensorNode output:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">p</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Build finished in 0:00:00
Optimization finished in 0:00:00
Construction finished in 0:00:00
TensorNode input:
t: 0.001
x: [[ 0.5 -0.1]]
t: 0.002
x: [[ 0.5 -0.1]]
t: 0.003
x: [[ 0.5 -0.1]]
t: 0.004
x: [[ 0.5 -0.1]]
t: 0.0050000004
x: [[ 0.5 -0.1]]
t: 0.006
x: [[ 0.5 -0.1]]
t: 0.007
x: [[ 0.5 -0.1]]
t: 0.008
x: [[ 0.5 -0.1]]
t: 0.009000001
x: [[ 0.5 -0.1]]
t: 0.010000001
x: [[ 0.5 -0.1]]
TensorNode output:
[[ 0.501 -0.099]
 [ 0.502 -0.098]
 [ 0.503 -0.097]
 [ 0.504 -0.096]
 [ 0.505 -0.095]
 [ 0.506 -0.094]
 [ 0.507 -0.093]
 [ 0.508 -0.092]
 [ 0.509 -0.091]
 [ 0.51  -0.09 ]]
</pre></div></div>
</div>
<p>We can see, as we expect, that the input tensor <code class="docutils literal notranslate"><span class="pre">t</span></code> is reflecting the current simulation time over the 10 timesteps we executed, and <code class="docutils literal notranslate"><span class="pre">x</span></code> contains the value of the input Node that we connected to the TensorNode. And we can see in the probe data that the TensorNode is outputting the operation we defined in TensorFlow (<code class="docutils literal notranslate"><span class="pre">tf.add(t,</span> <span class="pre">x)</span></code>).</p>
<p>One point that can be important to keep in mind is that the main Nengo simulation graph is built within a <code class="docutils literal notranslate"><span class="pre">tf.while_loop</span></code> (that is what allows us to simulate and optimize a Nengo network over time). So the code that is defined within the TensorNode function will be executed within that <code class="docutils literal notranslate"><span class="pre">while_loop</span></code> context, which is usually what we want. However, sometimes we may want to write code that will execute outside the simulation loop (for example, code required to create the TensorNode’s
parameters). This can be achieved by passing a callable class to the TensorNode instead of a simple function. That callable class can optionally define <code class="docutils literal notranslate"><span class="pre">pre_build</span></code> and <code class="docutils literal notranslate"><span class="pre">post_build</span></code> methods. <code class="docutils literal notranslate"><span class="pre">pre_build</span></code> will be called before the main simulation loop is constructed. <code class="docutils literal notranslate"><span class="pre">post_build</span></code> will be called after everything else in the graph has been constructed and the simulation Session has been initialized (this allows the TensorNode to build functionality that depends on an active Session or the
Simulator random seed).</p>
<p>Here is a simple TensorNode that illustrates the different build stages:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Network</span><span class="p">()</span> <span class="k">as</span> <span class="n">net</span><span class="p">:</span>
    <span class="k">class</span> <span class="nc">TensorFunc</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">pre_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape_in</span><span class="p">,</span> <span class="n">shape_out</span><span class="p">):</span>
            <span class="c1"># shape_in and shape_out are the input and output shape of</span>
            <span class="c1"># the TensorNode</span>

            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;in pre_build, scope:&quot;</span><span class="p">,</span>
                  <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_name_scope</span><span class="p">())</span>

        <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
            <span class="c1"># this is the main tensornode function, equivalent to the</span>
            <span class="c1"># tensor_func discussed above</span>

            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;in build, scope:&quot;</span><span class="p">,</span>
                  <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_name_scope</span><span class="p">())</span>
            <span class="k">return</span> <span class="n">x</span>

        <span class="k">def</span> <span class="nf">post_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sess</span><span class="p">,</span> <span class="n">rng</span><span class="p">):</span>
            <span class="c1"># post_build is called after the Simulator&#39;s underlying</span>
            <span class="c1"># tf.Session is initialized, and that Session is passed</span>
            <span class="c1"># in as `sess`. `rng` is the Simulator&#39;s random number</span>
            <span class="c1"># generator.</span>

            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;in post_build, scope:&quot;</span><span class="p">,</span>
                  <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_name_scope</span><span class="p">())</span>

    <span class="n">a</span> <span class="o">=</span> <span class="n">nengo_dl</span><span class="o">.</span><span class="n">TensorNode</span><span class="p">(</span><span class="n">TensorFunc</span><span class="p">(),</span> <span class="n">size_in</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size_out</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># build the network</span>
<span class="k">with</span> <span class="n">nengo_dl</span><span class="o">.</span><span class="n">Simulator</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">sim</span><span class="p">:</span>
    <span class="k">pass</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
in pre_build, scope: SimTensorNodeBuilder
in build, scope: while/iteration_0/SimTensorNodeBuilder
in post_build, scope: SimTensorNodeBuilder_1
</pre></div></div>
</div>
<p>More details on TensorNode usage can be found in <a class="reference external" href="https://www.nengo.ai/nengo-dl/tensor-node.html">the user guide</a>.</p>
</div>
<div class="section" id="Deep-learning-parameter-optimization">
<h2>Deep learning parameter optimization<a class="headerlink" href="#Deep-learning-parameter-optimization" title="Permalink to this headline">¶</a></h2>
<p>NengoDL allows model parameters to be optimized via TensorFlow optimization algorithms, through the <code class="docutils literal notranslate"><span class="pre">Simulator.train</span></code> function. Returning to the autoencoder examples from the beginning of this tutorial, we’ll optimize those networks to encode MNIST digits.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># download MNIST dataset</span>
<span class="n">urlretrieve</span><span class="p">(</span><span class="s2">&quot;http://deeplearning.net/data/mnist/mnist.pkl.gz&quot;</span><span class="p">,</span>
            <span class="s2">&quot;mnist.pkl.gz&quot;</span><span class="p">)</span>
<span class="k">with</span> <span class="n">gzip</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;mnist.pkl.gz&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">_</span><span class="p">,</span> <span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
        <span class="n">f</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;latin1&quot;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>

<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
</div>
</div>
<p>In TensorFlow the training would be done something like:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">auto_graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="c1"># create placeholder for target values</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">(</span><span class="n">minibatch_size</span><span class="p">,</span> <span class="n">n_in</span><span class="p">))</span>

    <span class="c1"># compute loss (mean squared error)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">targets</span> <span class="o">-</span> <span class="n">tf_c</span><span class="p">))</span>

    <span class="c1"># apply optimizer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">RMSPropOptimizer</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="n">opt_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">auto_graph</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

    <span class="c1"># run training loop</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">),</span> <span class="n">minibatch_size</span><span class="p">):</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">opt_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
                <span class="n">tf_a</span><span class="p">:</span> <span class="n">train_data</span><span class="p">[</span><span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">minibatch_size</span><span class="p">],</span>
                <span class="n">targets</span><span class="p">:</span> <span class="n">train_data</span><span class="p">[</span><span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">minibatch_size</span><span class="p">]})</span>

    <span class="c1"># evaluate performance on test set</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span>
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
            <span class="n">tf_a</span><span class="p">:</span> <span class="n">test_data</span><span class="p">[</span><span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">minibatch_size</span><span class="p">],</span>
            <span class="n">targets</span><span class="p">:</span> <span class="n">test_data</span><span class="p">[</span><span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">minibatch_size</span><span class="p">]})</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">),</span> <span class="n">minibatch_size</span><span class="p">)])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;error:&quot;</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>

    <span class="c1"># display example output</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf_c</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">tf_a</span><span class="p">:</span> <span class="n">test_data</span><span class="p">[:</span><span class="n">minibatch_size</span><span class="p">]})</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
error: 0.013011572
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_from-tensorflow_29_1.png" src="../_images/examples_from-tensorflow_29_1.png" />
</div>
</div>
<p>Before running the same training in NengoDL, we’ll change the Nengo model parameters to more closely match the TensorFlow network (we omitted these details in the original presentation to keep things simple).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># set initial neuron gains to 1 and biases to 0</span>
<span class="k">for</span> <span class="n">ens</span> <span class="ow">in</span> <span class="n">auto_net</span><span class="o">.</span><span class="n">all_ensembles</span><span class="p">:</span>
    <span class="n">ens</span><span class="o">.</span><span class="n">gain</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">dists</span><span class="o">.</span><span class="n">Choice</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ens</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">dists</span><span class="o">.</span><span class="n">Choice</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># disable synaptic filtering on all connections</span>
<span class="k">for</span> <span class="n">conn</span> <span class="ow">in</span> <span class="n">auto_net</span><span class="o">.</span><span class="n">all_connections</span><span class="p">:</span>
    <span class="n">conn</span><span class="o">.</span><span class="n">synapse</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
</div>
<p>We also need to modify the data slightly. As mentioned above, NengoDL simulations are essentially temporal, so data is described over time (indicating what the inputs/targets should be on each simulation timestep). So instead of the data having shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">n)</span></code>, it will have shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">n_steps,</span> <span class="pre">n)</span></code>. In this case we’ll just be training for a single timestep, but we still need to add that extra axis with length 1.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">train_data</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">test_data</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
</pre></div>
</div>
</div>
<p>Now we can run the NengoDL equivalent of the above TensorFlow training:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># define loss function (we could use the pre-defined</span>
<span class="c1"># `nengo_dl.obj.mse`, but we define it explicitly here</span>
<span class="c1"># for clarity)</span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">outputs</span> <span class="o">-</span> <span class="n">targets</span><span class="p">))</span>

<span class="k">with</span> <span class="n">nengo_dl</span><span class="o">.</span><span class="n">Simulator</span><span class="p">(</span><span class="n">auto_net</span><span class="p">,</span> <span class="n">minibatch_size</span><span class="o">=</span><span class="n">minibatch_size</span><span class="p">)</span> <span class="k">as</span> <span class="n">sim</span><span class="p">:</span>
    <span class="c1"># sim.train automatically adds the necessary elements to the</span>
    <span class="c1"># graph and runs the training loop</span>
    <span class="c1"># note: the probe acts as the placeholder to feed in target values</span>
    <span class="n">sim</span><span class="o">.</span><span class="n">train</span><span class="p">({</span><span class="n">nengo_a</span><span class="p">:</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">p_c</span><span class="p">:</span> <span class="n">train_data</span><span class="p">},</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">RMSPropOptimizer</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">),</span>
              <span class="n">objective</span><span class="o">=</span><span class="p">{</span><span class="n">p_c</span><span class="p">:</span> <span class="n">loss</span><span class="p">},</span> <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">)</span>

    <span class="c1"># evaluate performance on test set</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">sim</span><span class="o">.</span><span class="n">loss</span><span class="p">({</span><span class="n">nengo_a</span><span class="p">:</span> <span class="n">test_data</span><span class="p">,</span> <span class="n">p_c</span><span class="p">:</span> <span class="n">test_data</span><span class="p">},</span>
                     <span class="n">objective</span><span class="o">=</span><span class="p">{</span><span class="n">p_c</span><span class="p">:</span> <span class="n">loss</span><span class="p">})</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;error:&quot;</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>

    <span class="c1"># display example output</span>
    <span class="n">sim</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">{</span><span class="n">nengo_a</span><span class="p">:</span> <span class="n">test_data</span><span class="p">[:</span><span class="n">minibatch_size</span><span class="p">]})</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">p_c</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Build finished in 0:00:00
Optimization finished in 0:00:00
Construction finished in 0:00:00
Training finished in 0:00:25 (loss: 0.0124)
Calculation finished in 0:00:01
error: 0.012338359
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_from-tensorflow_35_1.png" src="../_images/examples_from-tensorflow_35_1.png" />
</div>
</div>
<p>More details on using <code class="docutils literal notranslate"><span class="pre">sim.train</span></code> can be found in <a class="reference external" href="https://www.nengo.ai/nengo-dl/training.html">the user guide</a>.</p>
</div>
<div class="section" id="NEF-parameter-optimization">
<h2>NEF parameter optimization<a class="headerlink" href="#NEF-parameter-optimization" title="Permalink to this headline">¶</a></h2>
<p>NengoDL also provides access to a different optimization method, the Neural Engineering Framework (NEF). This uses linear least-squares optimization to solve for optimal connection weights analytically, rather than using an iterative gradient-descent based algorithm. The advantage of the NEF is that it is very fast and general (for example, it does not require the network to be differentiable). The disadvantage is that it optimizes each set of connection weights individually (i.e., it cannot
jointly optimize across multiple layers).</p>
<p>The NEF optimization is accessed by setting the <code class="docutils literal notranslate"><span class="pre">function</span></code> argument on a <code class="docutils literal notranslate"><span class="pre">nengo.Connection</span></code>. This specifies the function that we would like those connection weights to approximate. In addition, in previous examples you may have noticed that we were forming Connections using <code class="docutils literal notranslate"><span class="pre">ensemble.neurons</span></code> (rather than <code class="docutils literal notranslate"><span class="pre">ensemble</span></code>). Using <code class="docutils literal notranslate"><span class="pre">ensemble.neurons</span></code> specifies that we want to form a direct connection between ensemble neurons, without applying the NEF optimization. So when we want to use the
<code class="docutils literal notranslate"><span class="pre">function</span></code> argument, the <code class="docutils literal notranslate"><span class="pre">Connection</span></code> source object should be an <code class="docutils literal notranslate"><span class="pre">ensemble</span></code>, not <code class="docutils literal notranslate"><span class="pre">ensemble.neurons</span></code>. For example, we could use the NEF to create a network to approximate the function <span class="math notranslate nohighlight">\(sin(x^2)\)</span>:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Network</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">as</span> <span class="n">net</span><span class="p">:</span>
    <span class="c1"># input node outputting a random signal for x</span>
    <span class="n">inpt</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Node</span><span class="p">(</span><span class="n">nengo</span><span class="o">.</span><span class="n">processes</span><span class="o">.</span><span class="n">WhiteSignal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">rms</span><span class="o">=</span><span class="mf">0.3</span><span class="p">))</span>

    <span class="c1"># first ensemble, will compute x^2</span>
    <span class="n">ens0</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Ensemble</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># second ensemble, will compute sin(x^2)</span>
    <span class="n">ens1</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Ensemble</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># output node</span>
    <span class="n">outpt</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Node</span><span class="p">(</span><span class="n">size_in</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># connect input to first ensemble</span>
    <span class="n">nengo</span><span class="o">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">inpt</span><span class="p">,</span> <span class="n">ens0</span><span class="p">)</span>

    <span class="c1"># connect first to second ensemble, solve for weights</span>
    <span class="c1"># to approximate the square function</span>
    <span class="n">nengo</span><span class="o">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">ens0</span><span class="p">,</span> <span class="n">ens1</span><span class="p">,</span> <span class="n">function</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">)</span>

    <span class="c1"># connect second ensemble to output, solve for weights</span>
    <span class="c1"># to approximate the sin function</span>
    <span class="n">nengo</span><span class="o">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">ens1</span><span class="p">,</span> <span class="n">outpt</span><span class="p">,</span> <span class="n">function</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">)</span>

    <span class="c1"># add a probe on the input and output</span>
    <span class="n">inpt_p</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Probe</span><span class="p">(</span><span class="n">inpt</span><span class="p">)</span>
    <span class="n">outpt_p</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Probe</span><span class="p">(</span><span class="n">outpt</span><span class="p">)</span>

<span class="k">with</span> <span class="n">nengo_dl</span><span class="o">.</span><span class="n">Simulator</span><span class="p">(</span><span class="n">net</span><span class="p">)</span> <span class="k">as</span> <span class="n">sim</span><span class="p">:</span>
    <span class="n">sim</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">trange</span><span class="p">(),</span> <span class="n">sim</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">inpt_p</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">trange</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">inpt_p</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;sin(x^2)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">trange</span><span class="p">(),</span> <span class="n">sim</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">outpt_p</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;output&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Build finished in 0:00:00
Optimization finished in 0:00:00
Construction finished in 0:00:00
Simulation finished in 0:00:00
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_from-tensorflow_38_1.png" src="../_images/examples_from-tensorflow_38_1.png" />
</div>
</div>
<p>The NEF optimization can be used in combination with the deep learning optimization methods. For example, we could optimize some parameters with the NEF and others with <code class="docutils literal notranslate"><span class="pre">sim.train</span></code> (see <a class="reference external" href="https://www.nengo.ai/nengo-dl/examples/from-nengo.html">this example</a>). Or we could initialize each set of connection weights individually with the NEF, and then further refine them with end-to-end training via <code class="docutils literal notranslate"><span class="pre">sim.train</span></code>. As always, the overall theme is that NengoDL allows us to use whichever method is
most appropriate for a particular goal.</p>
<p>See <a class="reference external" href="https://www.nengo.ai/nengo/examples/advanced/nef_summary.html">this example</a> for a deeper introduction to the principles of the NEF.</p>
</div>
<div class="section" id="Running-on-neuromorphic-hardware">
<h2>Running on neuromorphic hardware<a class="headerlink" href="#Running-on-neuromorphic-hardware" title="Permalink to this headline">¶</a></h2>
<p>Neuromorphic hardware is specialized compute hardware designed to simulate neuromorphic networks quickly/efficiently. However, often it is difficult to program this custom hardware, and it requires writing custom code for each neuromorphic platform. One of the primary design goals of Nengo is to alleviate these challenges, by providing a single API that can be used to build networks across many different neuromorphic platforms.</p>
<p>The idea is that the front-end network construction code is the same (<code class="docutils literal notranslate"><span class="pre">Networks</span></code>, <code class="docutils literal notranslate"><span class="pre">Nodes</span></code>, <code class="docutils literal notranslate"><span class="pre">Ensembles</span></code>, <code class="docutils literal notranslate"><span class="pre">Connections</span></code>, and <code class="docutils literal notranslate"><span class="pre">Probes</span></code>), and then each platform has its own <code class="docutils literal notranslate"><span class="pre">Simulator</span></code> class (the back-end) that compiles and executes that network definition for some compute platform. This provides a consistent interface so that we only need to write code once and can then run that network on novel hardware platforms with no additional effort. For example, we could take the network from
above and simulate it on different hardware platforms:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># run on a standard CPU</span>
<span class="k">with</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Simulator</span><span class="p">(</span><span class="n">net</span><span class="p">)</span> <span class="k">as</span> <span class="n">sim</span><span class="p">:</span>
    <span class="n">sim</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># run on Loihi neuromorphic hardware</span>
<span class="c1"># (requires https://www.nengo.ai/nengo-loihi/)</span>
<span class="c1"># with nengo_loihi.Simulator(net) as sim:</span>
<span class="c1">#     sim.run(1.0)</span>

<span class="c1"># run on SpiNNaker neuromorphic hardware</span>
<span class="c1"># (requires https://github.com/project-rig/nengo_spinnaker)</span>
<span class="c1"># with nengo_spinnaker.Simulator(net) as sim:</span>
<span class="c1">#     sim.run(1.0)</span>

<span class="c1"># run on any OpenCL-compatible hardware</span>
<span class="c1"># (requires https://github.com/nengo/nengo-ocl)</span>
<span class="c1"># with nengo_ocl.Simulator(net) as sim:</span>
<span class="c1">#     sim.run(1.0)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">trange</span><span class="p">(),</span> <span class="n">sim</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">inpt_p</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">trange</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">inpt_p</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;sin(x^2)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sim</span><span class="o">.</span><span class="n">trange</span><span class="p">(),</span> <span class="n">sim</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">outpt_p</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;output&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/home/travis/miniconda/envs/test/lib/python3.6/site-packages/nengo/builder/optimizer.py:636: UserWarning: Skipping some optimization steps because SciPy is not installed. Installing SciPy may result in faster simulations.
  warnings.warn(&#34;Skipping some optimization steps because SciPy is &#34;
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<div id="6379f55b-1deb-47f7-8d1a-603099fc190e" style="
    width: 100%;
    border: 1px solid #cfcfcf;
    border-radius: 4px;
    text-align: center;
    position: relative;">
  <div class="pb-text" style="
      position: absolute;
      width: 100%;">
    0%
  </div>
  <div class="pb-fill" style="
      background-color: #bdd2e6;
      width: 0%;">
    <style type="text/css" scoped="scoped">
        @keyframes pb-fill-anim {
            0% { background-position: 0 0; }
            100% { background-position: 100px 0; }
        }
    </style>
    &nbsp;
  </div>
</div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div></div>
<script type="text/javascript">
var element = document.currentScript.previousSibling.previousSibling;

              (function () {
                  var root = document.getElementById('6379f55b-1deb-47f7-8d1a-603099fc190e');
                  var text = root.getElementsByClassName('pb-text')[0];
                  var fill = root.getElementsByClassName('pb-fill')[0];

                  text.innerHTML = 'Build finished in 0:00:01.';

            fill.style.width = '100%';
            fill.style.animation = 'pb-fill-anim 2s linear infinite';
            fill.style.backgroundSize = '100px 100%';
            fill.style.backgroundImage = 'repeating-linear-gradient(' +
                '90deg, #bdd2e6, #edf2f8 40%, #bdd2e6 80%, #bdd2e6)';


                fill.style.animation = 'none';
                fill.style.backgroundImage = 'none';

              })();

</script></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<div id="7220d23d-7600-4410-96a6-222408f2d7b2" style="
    width: 100%;
    border: 1px solid #cfcfcf;
    border-radius: 4px;
    text-align: center;
    position: relative;">
  <div class="pb-text" style="
      position: absolute;
      width: 100%;">
    0%
  </div>
  <div class="pb-fill" style="
      background-color: #bdd2e6;
      width: 0%;">
    <style type="text/css" scoped="scoped">
        @keyframes pb-fill-anim {
            0% { background-position: 0 0; }
            100% { background-position: 100px 0; }
        }
    </style>
    &nbsp;
  </div>
</div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div></div>
<script type="text/javascript">
var element = document.currentScript.previousSibling.previousSibling;

              (function () {
                  var root = document.getElementById('7220d23d-7600-4410-96a6-222408f2d7b2');
                  var text = root.getElementsByClassName('pb-text')[0];
                  var fill = root.getElementsByClassName('pb-fill')[0];

                  text.innerHTML = 'Simulation finished in 0:00:01.';

            if (100.0 > 0.) {
                fill.style.transition = 'width 0.1s linear';
            } else {
                fill.style.transition = 'none';
            }

            fill.style.width = '100.0%';
            fill.style.animation = 'none';
            fill.style.backgroundImage = 'none'


                fill.style.animation = 'none';
                fill.style.backgroundImage = 'none';

              })();

</script></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/examples_from-tensorflow_41_5.png" src="../_images/examples_from-tensorflow_41_5.png" />
</div>
</div>
<p>We have commented out the different backends above because they require extra installation steps, but if you are running this example yourself you can install any of those backends (or more) and uncomment that code to see the same network running on that new hardware platform. Note that we can think of NengoDL as a TensorFlow back-end (among other things); it takes a standard Nengo network, and simulates it using TensorFlow.</p>
<p>We can take advantage of this cross-platform compatibility to effectively incorporate NengoDL’s deep learning functionality into any other Nengo back-end. We build our Network, optimize it in NengoDL, save the optimized model parameters back into the Network definition, and then simulate that optimized Network in a different back-end. See <a class="reference external" href="https://www.nengo.ai/nengo-loihi/examples/mnist_convnet.html">this example in nengo-loihi</a>, where a spiking network is optimized in NengoDL and then
deployed on Loihi.</p>
</div>
<div class="section" id="Conclusion">
<h2>Conclusion<a class="headerlink" href="#Conclusion" title="Permalink to this headline">¶</a></h2>
<p>In this tutorial we have demonstrated how to translate TensorFlow concepts into NengoDL, including network construction, execution, and optimization. We have also discussed how to use TensorNodes to combine TensorFlow and Nengo code, and introduced some of the unique features of Nengo (such as NEF optimization and neuromorphic cross-platform execution). However, there is much more functionality in NengoDL than we are able to introduce here; check out the <a class="reference external" href="https://www.nengo.ai/nengo-dl/user-guide.html">user
guide</a> or <a class="reference external" href="https://www.nengo.ai/nengo-dl/examples.html">other examples</a> for more information. If you would like more information on how NengoDL is implemented under the hood using TensorFlow, check out the <a class="reference external" href="https://arxiv.org/abs/1805.11144">white paper</a>.</p>
</div>
</div>


        </div>
      </div>
    </div>
  </div>
</div><footer class="footer">
  <div class="w-container">
    <div class="w-clearfix">
      <div class="w-row mobileAlign">
        <div class="w-col w-col-2 footer-menu">
          <p class="menu-Label">ABR, Inc.</p>
          <ul class="vertical-menu">
            <li class="footer-listitem-new"><a href="https://appliedbrainresearch.com/products/">Products</a></li>
            <li class="footer-listitem-new "><a href="https://appliedbrainresearch.com/services/">Services</a></li>
            <li class="footer-listitem-new"><a href="https://appliedbrainresearch.com/research/">Research</a></li>
          </ul>
        </div>
        <div class="w-col w-col-2 footer-menu">
          <p class="menu-Label">About Us</p>
          <ul class="vertical-menu">
            <li class="footer-listitem-new"><a href="https://appliedbrainresearch.com/about-us/">Contact</a></li>
            <li class="footer-listitem-new"><a href="https://appliedbrainresearch.com/about-us/#team">Team</a></li>
            <li class="footer-listitem-new"><a href="https://appliedbrainresearch.com/about-us/#culture">Culture</a></li>
          </ul>
        </div>
        <a class="brand footer-brand" href="https://appliedbrainresearch.com">
          <img src="https://appliedbrainresearch.com/img/logo-light-notext.svg" width="80">
        </a>
      </div>
    </div>
  </div>
</footer>
  </body>
</html>